{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool_depth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "clus_type = 'Cluster Aggregate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_sub(sub, index_of_ad, len_answ, len_id):\n",
    "    agg = []\n",
    "    entries = []\n",
    "    for entry in sub:\n",
    "        if type(entry) != list:\n",
    "            agg.append(entry)\n",
    "        elif len(entry) != len_answ:\n",
    "            agg.append(entry)\n",
    "        elif len(entry[index_of_ad]) != id_length:\n",
    "            agg.append(entry)\n",
    "        else:\n",
    "            entries.append(entry)\n",
    "        \n",
    "    return [agg, entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AvgPrecision(gt, sub, index_of_ad, index_of_score):\n",
    "    score = 100\n",
    "    seen = []\n",
    "    rel_docs = len(gt)\n",
    "    num_hits = 0\n",
    "    num_retrieved = 0\n",
    "    cumpre = 0\n",
    "    for entry in sub:\n",
    "        num_retrieved += 1\n",
    "        ad_id = entry[index_of_ad]\n",
    "        # excessive checking of dupes\n",
    "        if ad_id in seen:\n",
    "            print \"DUPLICATE\"\n",
    "            return None\n",
    "        seen.append(ad_id)\n",
    "        # excessive checking of order\n",
    "        if entry[index_of_score] > score:\n",
    "            print \"ORDER PROBLEM\"\n",
    "            return None\n",
    "        score = entry[index_of_score]\n",
    "        if ad_id in gt:\n",
    "            num_hits += 1\n",
    "            cumpre += num_hits/float(num_retrieved)\n",
    "            \n",
    "    return cumpre/rel_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sub(sub, index_of_ad, index_of_score, expected_length):\n",
    "    seen = []\n",
    "    uniqs = []\n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    sub.sort(key=operator.itemgetter(index_of_score), reverse=True)\n",
    "    for entry in sub:\n",
    "        if entry[index_of_score] > i:\n",
    "            print \"ORDERING PROBLEM\"\n",
    "            return None\n",
    "        i = entry[index_of_score]\n",
    "        #else:\n",
    "        #    print type(entry)\n",
    "    # de-dupe\n",
    "    for entry in sub:\n",
    "        if entry[index_of_ad] not in seen:\n",
    "            uniqs.append(entry)\n",
    "            seen.append(entry[index_of_ad])\n",
    "    # explicitly sort by score after de-duping\n",
    "    uniqs.sort(key=operator.itemgetter(index_of_score), reverse=True)\n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    for entry in uniqs:\n",
    "        if entry[index_of_score] > i:\n",
    "            print \"ORDERING PROBLEM\"\n",
    "            return None\n",
    "        i = entry[index_of_score]\n",
    "    return uniqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATUS: MUST UPDATE CORRECT ANSWERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAKE SURE YOU UPDATED TO RIGHT FILE\n"
     ]
    }
   ],
   "source": [
    "round_one_dd_clustering_file =  '../../../data/annotation_prep/dd_clustering/FIRST_ROUND_seeds2cluster_ads.json'\n",
    "round_two_dd_clustering_file =  '../../../data/annotation_prep/dd_clustering/SECOND_ROUND_seeds2cluster_ads.json'\n",
    "#dd_clustering_file =  '../../../data/annotation_prep/dd_clustering/FINAL_seeds2cluster_ads.json'\n",
    "\n",
    "print \"MAKE SURE YOU UPDATED TO RIGHT FILE\"\n",
    "\n",
    "questions_file =  '../../../data/annotation_prep/dd_clustering/FIRST_ROUND_chosen_questions.json'\n",
    "\n",
    "with open(round_one_dd_clustering_file, 'r') as f:\n",
    "    round_one_seeds2cluster_ads = eval(f.read()) \n",
    "    \n",
    "with open(round_two_dd_clustering_file, 'r') as f:\n",
    "    round_two_seeds2cluster_ads = eval(f.read())   \n",
    "    \n",
    "with open(questions_file, 'r') as f:\n",
    "    questions = eval(f.read())\n",
    "    \n",
    "seeds2cluster_ads = {}\n",
    "for seed in round_one_seeds2cluster_ads.keys():\n",
    "    temp = round_one_seeds2cluster_ads[seed]\n",
    "    temp.extend(round_two_seeds2cluster_ads[seed])\n",
    "    temp = list(set(temp))\n",
    "    seeds2cluster_ads[seed] = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question_ids = questions['NYU'][clus_type].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To avoid confusing dependecies, we'll get submission data from raw submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_path = '../../../data/team_submissions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers['NYU'] = {}\n",
    "answers['NYU'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = submission_path + 'Georgetown/DomainDiscovery/NYU_aggregate.json'\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "for qid in question_ids:\n",
    "    submissions = []\n",
    "    for entry in data:\n",
    "        # We are only evaluating some of them\n",
    "        if entry['id'] == qid:\n",
    "            # NEW\n",
    "            parsed_submission = parse_sub(entry['answer'], id_pos, ans_length, id_length)\n",
    "            agg_answer = parsed_submission[0]\n",
    "            submissions = clean_sub(parsed_submission[1], id_pos, score_pos, ans_length)\n",
    "    # Reconfirming no dupes\n",
    "    if len(submissions) != len(list(set(tuple(i) for i in submissions))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    # Now that submissions are sorted, only take top N\n",
    "    if len(submissions) > pool_depth:\n",
    "        submissions = submissions[:pool_depth]\n",
    "        print \"POOL DEPTH REACHED\"\n",
    "    # NEW\n",
    "    answers['NYU'][clus_type][qid] = {}\n",
    "    answers['NYU'][clus_type][qid]['agg_answer'] = agg_answer\n",
    "    answers['NYU'][clus_type][qid]['sub'] = submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers['HG'] = {}\n",
    "answers['HG'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POOL DEPTH REACHED\n",
      "POOL DEPTH REACHED\n",
      "POOL DEPTH REACHED\n",
      "POOL DEPTH REACHED\n",
      "POOL DEPTH REACHED\n"
     ]
    }
   ],
   "source": [
    "file_path = submission_path + 'Georgetown/DomainDiscovery/Georgetown_Submission_2_8/HG/HG_post_aggregate.json'\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "for qid in question_ids:\n",
    "    submissions = []\n",
    "    for entry in data:\n",
    "        # We are only evaluating some of them\n",
    "        if entry['id'] == qid:\n",
    "            parsed_submission = parse_sub(entry['answer'], id_pos, ans_length, id_length)\n",
    "            agg_answer = parsed_submission[0]\n",
    "            submissions = clean_sub(parsed_submission[1], id_pos, score_pos, ans_length)\n",
    "    # Reconfirming no dupes\n",
    "    if len(submissions) != len(list(set(tuple(i) for i in submissions))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    # Now that submissions are sorted, only take top N\n",
    "    if len(submissions) > pool_depth:\n",
    "        submissions = submissions[:pool_depth]\n",
    "        print \"POOL DEPTH REACHED\"\n",
    "    answers['HG'][clus_type][qid] = {}\n",
    "    answers['HG'][clus_type][qid]['agg_answer'] = agg_answer\n",
    "    answers['HG'][clus_type][qid]['sub'] = submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers['JPL'] = {}\n",
    "answers['JPL'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = submission_path + 'Georgetown/DomainDiscovery/Georgetown_Submission_2_8/JPL/JPL_post_aggregate.json'\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "for qid in question_ids:\n",
    "    submissions = []\n",
    "    for entry in data:\n",
    "        # We are only evaluating some of them\n",
    "        if entry['id'] == qid:\n",
    "            parsed_submission = parse_sub(entry['answer'], id_pos, ans_length, id_length)\n",
    "            agg_answer = parsed_submission[0]\n",
    "            submissions = clean_sub(parsed_submission[1], id_pos, score_pos, ans_length)\n",
    "    # Reconfirming no dupes\n",
    "    if len(submissions) != len(list(set(tuple(i) for i in submissions))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    # Now that submissions are sorted, only take top N\n",
    "    if len(submissions) > pool_depth:\n",
    "        submissions = submissions[:pool_depth]\n",
    "        print \"POOL DEPTH REACHED\"\n",
    "    answers['JPL'][clus_type][qid] = {}\n",
    "    answers['JPL'][clus_type][qid]['agg_answer'] = agg_answer\n",
    "    answers['JPL'][clus_type][qid]['sub'] = submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "results[clus_type] = {}\n",
    "\n",
    "for dataset in answers.keys():\n",
    "    results[clus_type][dataset] = {}\n",
    "    for quest in answers[dataset][clus_type].keys():\n",
    "        results[clus_type][dataset][quest] = {}\n",
    "        # NEW\n",
    "        submission = answers[dataset][clus_type][quest]['sub']\n",
    "        seed = questions[dataset][clus_type][quest]['seed']\n",
    "        relevant_ads = seeds2cluster_ads[seed]\n",
    "        results[clus_type][dataset][quest] = {}\n",
    "        if len(relevant_ads) != 0:\n",
    "            results[clus_type][dataset][quest]['avg_precision'] = AvgPrecision(relevant_ads, submission, id_pos, score_pos)\n",
    "        else:\n",
    "            results[clus_type][dataset][quest]['avg_precision'] = 'NA'\n",
    "            print seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is known that aylinakkayam@gmail.com produces no relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file = 'Georgetown_DD_Cluster_Agg_Results_REWRITE.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(results, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow:\n",
    "Explicitly check for doc ID length, given agg answer type\n",
    "Separate submissions by docs, and aggs\n",
    "Change all input files\n",
    "Change index of positions\n",
    "Acount for missing no relevant docs for aylinakkayam@gmail.com relevant\n",
    "remove clus type references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-21-5cdefe165eb4>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-5cdefe165eb4>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    #print ' '\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['HG']:\n",
    "    print dataset\n",
    "    for question in answers[dataset][clus_type].keys():\n",
    "        #print question\n",
    "        #print answers[dataset][clus_type][question]['sub']\n",
    "        #print answers[dataset][clus_type][question]['agg_answer']\n",
    "        #print ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
