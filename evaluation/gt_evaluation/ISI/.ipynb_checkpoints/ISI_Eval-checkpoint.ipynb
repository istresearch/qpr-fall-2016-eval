{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt_path = '../../../data/posted_GT/gt_answer_key/'\n",
    "# New\n",
    "new_pf_answ_file = '../../../data/posted_GT/ground_truth_pf_submissions_relevant_V4.json'\n",
    "agg_gt_answ_file = '../../../data/aggregate_gt/agg_answer_key.json'\n",
    "agg_metric_file = '../../../data/aggregate_gt/agg_metric_parameters.json'\n",
    "sub_path = '../../../data/team_submissions/ISI/GroundTruth/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_sub(sub, index_of_ad, index_of_score):\n",
    "    seen = []\n",
    "    uniqs = []\n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    for entry in sub:\n",
    "        # skip over aggregate single answer\n",
    "        if type(entry) == list:\n",
    "            if entry[index_of_score] > i:\n",
    "                print \"ORDERING PROBLEM\"\n",
    "                return None\n",
    "            i = entry[index_of_score]\n",
    "        #else:\n",
    "        #    print type(entry)\n",
    "    # de-dupe\n",
    "    for entry in sub:\n",
    "        # skip over aggregate single answer\n",
    "        if type(entry) == list:\n",
    "            if entry[index_of_ad] not in seen:\n",
    "                uniqs.append(entry)\n",
    "                seen.append(entry[index_of_ad])\n",
    "    # explicitly sort by score after de-duping\n",
    "    uniqs.sort(key=operator.itemgetter(index_of_score), reverse=True)\n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    for entry in uniqs:\n",
    "        if entry[index_of_score] > i:\n",
    "            print \"ORDERING PROBLEM\"\n",
    "            return None\n",
    "        i = entry[index_of_score]\n",
    "    return uniqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AvgPrecision(gt, sub, index_of_ad, index_of_score):\n",
    "    score = 100\n",
    "    seen = []\n",
    "    rel_docs = len(gt)\n",
    "    num_hits = 0\n",
    "    num_retrieved = 0\n",
    "    cumpre = 0\n",
    "    for entry in sub:\n",
    "        num_retrieved += 1\n",
    "        ad_id = entry[index_of_ad]\n",
    "        # excessive checking of dupes\n",
    "        if ad_id in seen:\n",
    "            print \"DUPLICATE\"\n",
    "            return None\n",
    "        seen.append(ad_id)\n",
    "        # excessive checking of order\n",
    "        if entry[index_of_score] > score:\n",
    "            print \"ORDER PROBLEM\"\n",
    "            return None\n",
    "        score = entry[index_of_score]\n",
    "        if ad_id in gt:\n",
    "            num_hits += 1\n",
    "            cumpre += num_hits/float(num_retrieved)\n",
    "            \n",
    "    return cumpre/rel_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AggMetric(gt_answ, sub_answ, feature):\n",
    "    function = agg_metric_params[feature]['function']\n",
    "    amount = agg_metric_params[feature]['amount']\n",
    "    diff = gt_answ - sub_answ\n",
    "    if function == 'multiply':\n",
    "        if numpy.absolute(diff) <= (gt_answ * amount):\n",
    "            metric = 1\n",
    "        else:\n",
    "            metric = 0\n",
    "    elif function == 'add':\n",
    "        if numpy.absolute(diff) <= amount:\n",
    "            metric = 1\n",
    "        else:\n",
    "            metric = 0\n",
    "        \n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take average of binary \"correct\" or \"incorrect\" for all relevant ads found\n",
    "# ANY correct extraction for a relevant ad makes the entry for that\n",
    "# ad \"correct\" regardless of ranking of extraction\n",
    "def PFMetric(gt_key, sub_answers):\n",
    "    # Keep track of ranking\n",
    "    score = 100\n",
    "    tally = []\n",
    "    # Checking submission format again\n",
    "    for entry in sub_answers:\n",
    "        # Checking expected entry length\n",
    "        if len(entry) != 3:\n",
    "            print \"PROBLEM WITH ENTRY LENGTH\"\n",
    "        # Checking expected entry ranking\n",
    "        if entry[2] > score:\n",
    "            print \"PROBLEM WITH ORDER\"\n",
    "        score = entry[2]\n",
    "    # Inefficient\n",
    "    for rel_ad in gt_key.keys():\n",
    "        lower_ext = []\n",
    "        for entry in sub_answers:\n",
    "            if entry[1] == rel_ad:\n",
    "                # Collect all extractions for all occurences\n",
    "                # of relevant ad\n",
    "                lower_ext.append(str(entry[0]).lower())\n",
    "        # If relevant ad found\n",
    "        if len(lower_ext) > 0:\n",
    "            # Initiate metric to 0\n",
    "            match = 0\n",
    "            # Check collected extractions against gt answers   \n",
    "            # Allow for different case\n",
    "            for gt_ans in gt_key[rel_ad]['correct']:\n",
    "                if gt_ans.lower() in lower_ext:\n",
    "                    # Found match\n",
    "                    match = 1\n",
    "            tally.append(match)\n",
    "    # If any relevant ads were found\n",
    "    if len(tally) > 0:\n",
    "        return numpy.mean(tally)\n",
    "    # If no relevant ads found, PF metric is Not Applicable\n",
    "    else:\n",
    "        return 'NA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET EXTRACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extractions = 'ISI_ISI_only'\n",
    "#extractions = 'ISI_lattice_only'\n",
    "#extractions = 'ISI_ISI_and_lattice'\n",
    "\n",
    "if extractions == 'ISI_ISI_only':\n",
    "    sub_pf = sub_path + 'resubmission-isi-ground-truth-2016-11-30/post_point_fact_parsed_fixed_all_answers.json'\n",
    "    sub_id = sub_path + 'resubmission-isi-ground-truth-2016-11-30/post_cluster_identification-parsed_fixed_all_answers.json'\n",
    "    sub_facet = sub_path + 'resubmission-isi-ground-truth-2016-11-30/post_cluster_facet_parsed_fixed_all_answers.json'\n",
    "    sub_agg = sub_path + 'resubmission-isi-ground-truth-2016-11-30/post_aggregate_parsed_fixed_all_answers.json'\n",
    "    \n",
    "elif extractions == 'ISI_lattice_only':\n",
    "    sub_pf = sub_path + 'isi-submission-only-lattice-extractions/post_point_fact_parsed_fixed_all_answers.json'\n",
    "    sub_id = sub_path + 'isi-submission-only-lattice-extractions/post_cluster_identification-parsed_fixed_all_answers.json'\n",
    "    sub_facet = sub_path + 'isi-submission-only-lattice-extractions/post_cluster_facet_parsed_fixed_all_answers.json'\n",
    "    sub_agg = sub_path + 'isi-submission-only-lattice-extractions/post_aggregate_parsed_fixed_all_answers.json'\n",
    "    \n",
    "elif extractions == 'ISI_ISI_and_lattice':\n",
    "    sub_pf = sub_path + 'resubmission-dig-plus-lattice-extractions/post_point_fact_parsed_fixed_all_answers.json'\n",
    "    sub_id = sub_path + 'resubmission-dig-plus-lattice-extractions/post_cluster_identification-parsed_fixed_all_answers.json'\n",
    "    sub_facet = sub_path + 'resubmission-dig-plus-lattice-extractions/post_cluster_facet_parsed_fixed_all_answers.json'\n",
    "    sub_agg = sub_path + 'resubmission-dig-plus-lattice-extractions/post_aggregate_parsed_fixed_all_answers.json'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(sub_pf, 'r')\n",
    "pf_sub = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(sub_id, 'r')\n",
    "id_sub = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(sub_facet, 'r')\n",
    "facet_sub = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(sub_agg, 'r')\n",
    "agg_sub = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "Submissions = {}\n",
    "Submissions['Point Fact'] = pf_sub\n",
    "Submissions['Cluster Identification'] = id_sub\n",
    "Submissions['Cluster Facet'] = facet_sub\n",
    "Submissions['Cluster Aggregate'] = agg_sub\n",
    "\n",
    "del pf_sub\n",
    "del id_sub\n",
    "del facet_sub\n",
    "del agg_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pf_path = gt_path + 'pf_quest2ads.json'\n",
    "f = open(pf_path, 'r')\n",
    "pf_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "id_path = gt_path + 'id_quest2ads.json'\n",
    "f = open(id_path, 'r')\n",
    "id_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "facet_path = gt_path + 'facet_quest2ads.json'\n",
    "f = open(facet_path, 'r')\n",
    "facet_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "agg_path = gt_path + 'agg_quest2ads.json'\n",
    "f = open(agg_path, 'r')\n",
    "agg_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(new_pf_answ_file, 'r')\n",
    "pf_answ_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(agg_gt_answ_file, 'r')\n",
    "agg_answ_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(agg_metric_file, 'r')\n",
    "agg_metric_params = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "Ground_Truth = {}\n",
    "Ground_Truth['Point Fact'] = pf_gt\n",
    "Ground_Truth['Cluster Identification'] = id_gt\n",
    "Ground_Truth['Cluster Facet'] = facet_gt\n",
    "Ground_Truth['Cluster Aggregate'] = agg_gt\n",
    "\n",
    "# Incorporate extra relevant ads found by annotators\n",
    "for q_id in pf_answ_gt.keys():\n",
    "    Ground_Truth['Point Fact'][q_id].extend(pf_answ_gt[q_id].keys())\n",
    "    Ground_Truth['Point Fact'][q_id] = list(set(Ground_Truth['Point Fact'][q_id]))\n",
    "\n",
    "del pf_gt\n",
    "del id_gt\n",
    "del facet_gt\n",
    "del agg_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "protocol = {}\n",
    "protocol['Point Fact'] = {}\n",
    "protocol['Point Fact']['ad_idx'] = 1\n",
    "protocol['Point Fact']['score_idx'] = 2\n",
    "protocol['Point Fact']['type'] = ['Point Fact']\n",
    "protocol['Cluster Identification'] = {}\n",
    "protocol['Cluster Identification']['ad_idx'] = 1\n",
    "protocol['Cluster Identification']['score_idx'] = 2\n",
    "protocol['Cluster Identification']['type'] = ['Cluster Identification']\n",
    "protocol['Cluster Facet'] = {}\n",
    "protocol['Cluster Facet']['ad_idx'] = 1\n",
    "protocol['Cluster Facet']['score_idx'] = 2\n",
    "protocol['Cluster Facet']['type'] = ['Cluster Facet']\n",
    "protocol['Cluster Aggregate'] = {}\n",
    "protocol['Cluster Aggregate']['ad_idx'] = 1\n",
    "protocol['Cluster Aggregate']['score_idx'] = 2\n",
    "protocol['Cluster Aggregate']['type'] = ['MODE', 'MIN', 'MAX', 'AVG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AvgP = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for qtype in protocol.keys():\n",
    "    AvgP[qtype] = {}\n",
    "\n",
    "    sub_data = Submissions[qtype]\n",
    "    gt_data = Ground_Truth[qtype]\n",
    "\n",
    "    try:\n",
    "        del submissions\n",
    "    except:\n",
    "        pass\n",
    "    for sub in sub_data:\n",
    "        if sub['type'] not in protocol[qtype]['type']:\n",
    "            print sub['type']\n",
    "            print \"TROUBLE\"\n",
    "            print qtype\n",
    "        submissions = clean_sub(sub['answer'], protocol[qtype]['ad_idx'], protocol[qtype]['score_idx'])\n",
    "        q_id = sub['question_id']\n",
    "        if '-' in q_id:\n",
    "            q_id = q_id.split('-')[0]\n",
    "        gt = gt_data[q_id]\n",
    "        AvgP[qtype][q_id] = AvgPrecision(gt, submissions, protocol[qtype]['ad_idx'], protocol[qtype]['score_idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Question 94 from Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del AvgP['Cluster Aggregate']['94']\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NewEval = {}\n",
    "\n",
    "for qtype in AvgP.keys():\n",
    "    NewEval[qtype] = {}\n",
    "    for quest in AvgP[qtype].keys():\n",
    "        NewEval[qtype][quest] = {}\n",
    "        NewEval[qtype][quest]['avg_precision'] = AvgP[qtype][quest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Overall Aggregate Metric and Reformat Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for quest in Submissions['Cluster Aggregate']:\n",
    "    q_id = quest['question_id'].split('-')[0]\n",
    "    # Initiate metric at 0\n",
    "    metric = 0\n",
    "    # Skip submission for question 94\n",
    "    if q_id != '94':\n",
    "        if len(quest['answer']) > 0:\n",
    "            # Some may be empty strings\n",
    "            if len(quest['answer'][0]) > 0:\n",
    "                # Some answers have strings\n",
    "                if ',' in quest['answer'][0]:\n",
    "                    sub_answer = float(quest['answer'][0].split(',')[0])\n",
    "                else:\n",
    "                    sub_answer = float(quest['answer'][0])\n",
    "                metric = AggMetric(agg_answ_gt[q_id]['agg_answer'], sub_answer, agg_answ_gt[q_id]['feature'])\n",
    "        NewEval['Cluster Aggregate'][q_id]['agg_metric'] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Point Fact Answer Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for quest in Submissions['Point Fact']:\n",
    "    q_id = quest['question_id'].split('-')[0]\n",
    "    pf_metric = PFMetric(pf_answ_gt[q_id], quest['answer'])\n",
    "    NewEval['Point Fact'][q_id]['pf_metric'] = pf_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Avg Precision Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_file = extractions +'.json'\n",
    "with open(save_file, 'w') as outfile:\n",
    "    json.dump(NewEval, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Checking Units on Aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quest_file = '../../../questions/post_aggregate_V2.json'\n",
    "f = open(quest_file, 'r')\n",
    "questions = []\n",
    "for line in f:\n",
    "    entry = json.loads(line)\n",
    "    questions.append(entry)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 75\n",
    "print \"QUESTION\"\n",
    "print questions[i]['id']\n",
    "print questions[i]['question']\n",
    "print \" \"\n",
    "print \"SUBMISSION\"\n",
    "print Submissions['Cluster Aggregate'][i]['question_id']\n",
    "print Submissions['Cluster Aggregate'][i]['answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
