{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_pure_agg_gt_values.json  pure_agg_gt_quest2ads.json\r\n",
      "pure_agg_gt_answer_key.json\r\n"
     ]
    }
   ],
   "source": [
    "ls ../../data/pure_agg_gt/saved_dictionaries/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_sub(sub, index_of_ad, index_of_score, expected_length):\n",
    "    seen = []\n",
    "    uniqs = []\n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    sub.sort(key=operator.itemgetter(index_of_score), reverse=True)\n",
    "    for entry in sub:\n",
    "        # skip over aggregate single answer\n",
    "        # this method made more complicated due\n",
    "        # to inconsistent submission format\n",
    "        if len(entry) == expected_length:\n",
    "            if entry[index_of_score] > i:\n",
    "                print \"ORDERING PROBLEM\"\n",
    "                return None\n",
    "            i = entry[index_of_score]\n",
    "        #else:\n",
    "        #    print type(entry)\n",
    "    # de-dupe\n",
    "    for entry in sub:\n",
    "        # skip over aggregate single answer\n",
    "        # this method made more complicated due\n",
    "        # to inconsistent submission format\n",
    "        if len(entry) == expected_length:\n",
    "            if entry[index_of_ad] not in seen:\n",
    "                uniqs.append(entry)\n",
    "                seen.append(entry[index_of_ad])\n",
    "    # explicitly sort by score after de-duping\n",
    "    uniqs.sort(key=operator.itemgetter(index_of_score), reverse=True)\n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    for entry in uniqs:\n",
    "        if entry[index_of_score] > i:\n",
    "            print \"ORDERING PROBLEM\"\n",
    "            return None\n",
    "        i = entry[index_of_score]\n",
    "    return uniqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AvgPrecision(gt, sub, index_of_ad, index_of_score):\n",
    "    score = 100\n",
    "    seen = []\n",
    "    rel_docs = len(gt)\n",
    "    num_hits = 0\n",
    "    num_retrieved = 0\n",
    "    cumpre = 0\n",
    "    for entry in sub:\n",
    "        num_retrieved += 1\n",
    "        ad_id = entry[index_of_ad]\n",
    "        # excessive checking of dupes\n",
    "        if ad_id in seen:\n",
    "            print \"DUPLICATE\"\n",
    "            return None\n",
    "        seen.append(ad_id)\n",
    "        # excessive checking of order\n",
    "        if entry[index_of_score] > score:\n",
    "            print \"ORDER PROBLEM\"\n",
    "            return None\n",
    "        score = entry[index_of_score]\n",
    "        if ad_id in gt:\n",
    "            num_hits += 1\n",
    "            cumpre += num_hits/float(num_retrieved)\n",
    "            \n",
    "    return cumpre/rel_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AggMetric(gt_answ, sub_answ, feature):\n",
    "    function = pure_agg_metric_params[feature]['function']\n",
    "    amount = pure_agg_metric_params[feature]['amount']\n",
    "    \n",
    "    if feature in ['price', 'height', 'weight', 'age']:\n",
    "        sub_answ = float(sub_answ)\n",
    "        \n",
    "        # Allow for centimeter height\n",
    "        if feature == 'height':\n",
    "            if sub_answ > 100:\n",
    "                sub_answ *= 0.393701\n",
    "                \n",
    "        # Allow for kilogram weight\n",
    "        if feature == 'weight':\n",
    "            if sub_answ < 90:\n",
    "                sub_answ *= 2.20462\n",
    "                \n",
    "        diff = gt_answ - sub_answ\n",
    "        if function == 'multiply':\n",
    "            if numpy.absolute(diff) <= (gt_answ * amount):\n",
    "                metric = 1\n",
    "            else:\n",
    "                metric = 0\n",
    "        elif function == 'add':\n",
    "            if numpy.absolute(diff) <= amount:\n",
    "                metric = 1\n",
    "            else:\n",
    "                metric = 0\n",
    "    if function == 'match':\n",
    "        if gt_answ in sub_answ.lower():\n",
    "            metric = 1\n",
    "        else:\n",
    "            metric = 0\n",
    "            \n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pure_agg_submission_files = {}\n",
    "pure_agg_submission_files['Georgetown'] = '../../data/team_submissions/Georgetown/GroundTruth/pure_agg_answer.json'\n",
    "pure_agg_submission_files['Uncharted'] = '../../data/team_submissions/Uncharted/GroundTruth/uncharted_GT_pure_aggregate_answers_JSON_LINES.json'\n",
    "pure_agg_submission_files['ISI'] = '../../data/team_submissions/ISI/GroundTruth/parsed_pure_aggregate_gt_v3_all_answers.jl'\n",
    "\n",
    "gt_file_path = '../../data/pure_agg_gt/saved_dictionaries/'\n",
    "gt_docs_file = gt_file_path + 'pure_agg_gt_quest2ads.json'\n",
    "gt_agg_file = gt_file_path + 'pure_agg_gt_answer_key.json'\n",
    "\n",
    "agg_parameters_file = gt_file_path + 'pure_agg_metric_parameters.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Submissions = {}\n",
    "\n",
    "for team in pure_agg_submission_files.keys():\n",
    "    pf_sub_file = pure_agg_submission_files[team]\n",
    "    subs = []\n",
    "    with open(pf_sub_file, 'r') as f:\n",
    "        for line in f:\n",
    "            temp = json.loads(line)\n",
    "            subs.append(temp)\n",
    "    Submissions[team] = subs\n",
    "    del subs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(gt_docs_file, 'r')\n",
    "pure_agg_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(gt_agg_file, 'r')\n",
    "pure_agg_answ_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(agg_parameters_file, 'r')\n",
    "pure_agg_metric_params = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NewEval = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When checking manually, note the implicit unit conversions performed in AggMetric()\n",
      " \n",
      "GEORGETOWN\n",
      " \n",
      "[75] 70.0 1\n",
      "[u'Mizz'] anna 0\n",
      "[u'5149091699'] 8137247050 0\n",
      "[19] 19 1\n",
      "[u'120'] 120 1\n",
      "[u'1'] 60 0\n",
      "[] 125 0\n",
      "[u'110'] 110 1\n",
      "[160] 63 1\n",
      "[28] 28 1\n",
      "[u'Churchland'] paige 0\n",
      "[u'Vanessa'] maria 0\n",
      "[u'black'] blonde 0\n",
      "[u'Savannah'] fort wayne 0\n",
      "[u'asian'] asian 1\n",
      " \n",
      "ISI\n",
      " \n",
      "[u'maria'] maria 1\n",
      "[u'140.054418273'] 70.0 0\n",
      "[u'22'] 19 0\n",
      "[u'brooklyn'] anna 0\n",
      "[u'8137247050'] 8137247050 1\n",
      "[u'103.314662334'] 120 0\n",
      "[u'60.0'] 60 1\n",
      "[u'56.0'] 125 1\n",
      "[u'49.0'] 110 1\n",
      "[u'160.003341795'] 63 1\n",
      "[u'june'] paige 0\n",
      "[u'asian'] asian 1\n",
      "[u''] blonde 0\n",
      "[u'28'] 28 1\n",
      "[u'fort wayne'] fort wayne 1\n",
      " \n",
      "UNCHARTED\n",
      " \n",
      "[u'maria'] maria 1\n",
      "[127.12121212121212] 70.0 0\n",
      "[19] 19 1\n",
      "[u'jessica'] anna 0\n",
      "[u'7865064729'] 8137247050 0\n",
      "[120] 120 1\n",
      "[60] 60 1\n",
      "[125] 125 1\n",
      "[110] 110 1\n",
      "[63] 63 1\n",
      "[u'paige'] paige 1\n",
      "[u'asian'] asian 1\n",
      "[u'brunette'] blonde 0\n",
      "[28] 28 1\n",
      "[u'Savannah,GA,USA'] fort wayne 0\n"
     ]
    }
   ],
   "source": [
    "print 'When checking manually, note the implicit unit conversions performed in AggMetric()'\n",
    "\n",
    "for team in Submissions.keys():\n",
    "\n",
    "    NewEval['Pure Aggregate'] = {}\n",
    "    \n",
    "    sub_data = Submissions[team]\n",
    "    gt_data = pure_agg_gt\n",
    "\n",
    "    try:\n",
    "        del submissions\n",
    "    except:\n",
    "        pass\n",
    "    for sub in sub_data:\n",
    "        submissions = clean_sub(sub['answers'], 1, 2, 3)\n",
    "\n",
    "        q_id = sub['question_id']\n",
    "        gt = gt_data[q_id]\n",
    "        NewEval['Pure Aggregate'][q_id] = {}\n",
    "        NewEval['Pure Aggregate'][q_id]['avg_precision'] = AvgPrecision(gt, submissions, 1, 2)\n",
    "        \n",
    "        # Calculate Aggregate Metric\n",
    "        if len(sub['aggregate']) == 1:\n",
    "            metric = AggMetric(pure_agg_answ_gt[q_id]['agg_answer'], sub['aggregate'][0], pure_agg_answ_gt[q_id]['feature'])\n",
    "        elif len(sub['aggregate']) > 1:\n",
    "            print \"MULTIPLE AGG SUBMISSIONS\"\n",
    "        elif len(sub['aggregate']) == 0:\n",
    "            metric = 0\n",
    "        NewEval['Pure Aggregate'][q_id]['agg_metric'] = metric\n",
    "    \n",
    "    # Manually check matching\n",
    "    print ' '\n",
    "    print team.upper()\n",
    "    print ' '  \n",
    "    for entry in Submissions[team]:\n",
    "        q_id = entry['question_id']\n",
    "        sub_agg = entry['aggregate']\n",
    "        gt_agg = pure_agg_answ_gt[q_id]['agg_answer']\n",
    "        points = NewEval['Pure Aggregate'][q_id]['agg_metric']\n",
    "        print '{0} {1} {2}'.format(sub_agg, gt_agg, points)\n",
    "        \n",
    "    output_file = team + '/' + team + '_pure_agg_gt_results_rewrite.json'\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(NewEval, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Evaluation Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_file = extractions +'.json'\n",
    "with open(save_file, 'w') as outfile:\n",
    "    json.dump(NewEval, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mGeorgetown\u001b[m\u001b[m/             Pure_Agg_GT_Eval.ipynb\r\n",
      "\u001b[34mISI\u001b[m\u001b[m/                    \u001b[34mUncharted\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = 3 \n",
    "news *= 2\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
