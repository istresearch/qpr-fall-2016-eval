{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt_path = '../../../data/posted_GT/gt_answer_key/'\n",
    "# New\n",
    "new_pf_answ_file = '../../../data/posted_GT/ground_truth_pf_submissions_relevant_V4.json'\n",
    "agg_gt_answ_file = '../../../data/aggregate_gt/agg_answer_key.json'\n",
    "agg_metric_file = '../../../data/aggregate_gt/agg_metric_parameters.json'\n",
    "sub_path = '../../../data/team_submissions/Georgetown/GroundTruth/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_sub(sub, index_of_ad, index_of_score, expected_length):\n",
    "    seen = []\n",
    "    uniqs = []\n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    for entry in sub:\n",
    "        # skip over aggregate single answer\n",
    "        # this method made more complicated due\n",
    "        # to inconsistent submission format\n",
    "        if len(entry) == expected_length:\n",
    "            if entry[index_of_score] > i:\n",
    "                print \"ORDERING PROBLEM\"\n",
    "                return None\n",
    "            i = entry[index_of_score]\n",
    "        #else:\n",
    "        #    print type(entry)\n",
    "    # de-dupe\n",
    "    for entry in sub:\n",
    "        # skip over aggregate single answer\n",
    "        # this method made more complicated due\n",
    "        # to inconsistent submission format\n",
    "        if len(entry) == expected_length:\n",
    "            if entry[index_of_ad] not in seen:\n",
    "                uniqs.append(entry)\n",
    "                seen.append(entry[index_of_ad])\n",
    "    # explicitly sort by score after de-duping\n",
    "    uniqs.sort(key=operator.itemgetter(index_of_score), reverse=True)\n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    for entry in uniqs:\n",
    "        if entry[index_of_score] > i:\n",
    "            print \"ORDERING PROBLEM\"\n",
    "            return None\n",
    "        i = entry[index_of_score]\n",
    "    return uniqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AvgPrecision(gt, sub, index_of_ad, index_of_score):\n",
    "    score = 100\n",
    "    seen = []\n",
    "    rel_docs = len(gt)\n",
    "    num_hits = 0\n",
    "    num_retrieved = 0\n",
    "    cumpre = 0\n",
    "    for entry in sub:\n",
    "        num_retrieved += 1\n",
    "        ad_id = entry[index_of_ad]\n",
    "        # excessive checking of dupes\n",
    "        if ad_id in seen:\n",
    "            print \"DUPLICATE\"\n",
    "            return None\n",
    "        seen.append(ad_id)\n",
    "        # excessive checking of order\n",
    "        if entry[index_of_score] > score:\n",
    "            print \"ORDER PROBLEM\"\n",
    "            return None\n",
    "        score = entry[index_of_score]\n",
    "        if ad_id in gt:\n",
    "            num_hits += 1\n",
    "            cumpre += num_hits/float(num_retrieved)\n",
    "            \n",
    "    return cumpre/rel_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AggMetric(gt_answ, sub_answ, feature):\n",
    "    scale_cutoff = agg_metric_params[feature]\n",
    "    diff = gt_answ - sub_answ\n",
    "    if numpy.absolute(diff) <= (gt_answ * scale_cutoff):\n",
    "        metric = 1\n",
    "    else:\n",
    "        metric = 0\n",
    "        \n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take average of binary \"correct\" or \"incorrect\" for all relevant ads found\n",
    "# ANY correct extraction for a relevant ad makes the entry for that\n",
    "# ad \"correct\" regardless of ranking of extraction\n",
    "def PFMetric(gt_key, sub_answers):\n",
    "    # Keep track of ranking\n",
    "    score = 100\n",
    "    tally = []\n",
    "    # Checking submission format again\n",
    "    for entry in sub_answers:\n",
    "        # Checking expected entry length\n",
    "        if len(entry) != 3:\n",
    "            print \"PROBLEM WITH ENTRY LENGTH\"\n",
    "        # Checking expected entry ranking\n",
    "        if entry[2] > score:\n",
    "            print \"PROBLEM WITH ORDER\"\n",
    "        score = entry[2]\n",
    "    # Inefficient\n",
    "    for rel_ad in gt_key.keys():\n",
    "        lower_ext = []\n",
    "        for entry in sub_answers:\n",
    "            if entry[1] == rel_ad:\n",
    "                # Collect all extractions for all occurences\n",
    "                # of relevant ad\n",
    "                lower_ext.append(str(entry[0]).lower())\n",
    "        # If relevant ad found\n",
    "        if len(lower_ext) > 0:\n",
    "            # Initiate metric to 0\n",
    "            match = 0\n",
    "            # Check collected extractions against gt answers   \n",
    "            # Allow for different case\n",
    "            for gt_ans in gt_key[rel_ad]['correct']:\n",
    "                if gt_ans.lower() in lower_ext:\n",
    "                    # Found match\n",
    "                    match = 1\n",
    "            tally.append(match)\n",
    "    # If any relevant ads were found\n",
    "    if len(tally) > 0:\n",
    "        return numpy.mean(tally)\n",
    "    # If no relevant ads found, PF metric is Not Applicable\n",
    "    else:\n",
    "        return 'NA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET EXTRACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extractions = 'Georgetown_Georgetown_only'\n",
    "\n",
    "if extractions == 'Georgetown_Georgetown_only':\n",
    "    sub_pf = sub_path + 'GT_pointfact.json'\n",
    "    sub_id = sub_path + 'GT_CI.json'\n",
    "    sub_facet = sub_path + 'GT_CF.json'\n",
    "    sub_agg = sub_path + 'GT_aggregate.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(sub_pf, 'r')\n",
    "pf_sub = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(sub_id, 'r')\n",
    "id_sub = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(sub_facet, 'r')\n",
    "facet_sub = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(sub_agg, 'r')\n",
    "agg_sub = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "Submissions = {}\n",
    "Submissions['Point Fact'] = pf_sub\n",
    "Submissions['Cluster Identification'] = id_sub\n",
    "Submissions['Cluster Facet'] = facet_sub\n",
    "Submissions['Cluster Aggregate'] = agg_sub\n",
    "\n",
    "del pf_sub\n",
    "del id_sub\n",
    "del facet_sub\n",
    "del agg_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pf_path = gt_path + 'pf_quest2ads.json'\n",
    "f = open(pf_path, 'r')\n",
    "pf_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "id_path = gt_path + 'id_quest2ads.json'\n",
    "f = open(id_path, 'r')\n",
    "id_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "facet_path = gt_path + 'facet_quest2ads.json'\n",
    "f = open(facet_path, 'r')\n",
    "facet_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "agg_path = gt_path + 'agg_quest2ads.json'\n",
    "f = open(agg_path, 'r')\n",
    "agg_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "# New\n",
    "f = open(agg_gt_answ_file, 'r')\n",
    "agg_answ_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(agg_metric_file, 'r')\n",
    "agg_metric_params = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(new_pf_answ_file, 'r')\n",
    "pf_answ_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "Ground_Truth = {}\n",
    "Ground_Truth['Point Fact'] = pf_gt\n",
    "Ground_Truth['Cluster Identification'] = id_gt\n",
    "Ground_Truth['Cluster Facet'] = facet_gt\n",
    "Ground_Truth['Cluster Aggregate'] = agg_gt\n",
    "\n",
    "# Incorporate extra relevant ads found by annotators\n",
    "for q_id in pf_answ_gt.keys():\n",
    "    Ground_Truth['Point Fact'][q_id].extend(pf_answ_gt[q_id].keys())\n",
    "    Ground_Truth['Point Fact'][q_id] = list(set(Ground_Truth['Point Fact'][q_id]))\n",
    "\n",
    "del pf_gt\n",
    "del id_gt\n",
    "del facet_gt\n",
    "del agg_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "protocol = {}\n",
    "protocol['Point Fact'] = {}\n",
    "protocol['Point Fact']['ad_idx'] = 1\n",
    "protocol['Point Fact']['score_idx'] = 2\n",
    "protocol['Point Fact']['type'] = ['Point Fact']\n",
    "protocol['Point Fact']['length'] = 3\n",
    "protocol['Cluster Identification'] = {}\n",
    "protocol['Cluster Identification']['ad_idx'] = 0\n",
    "protocol['Cluster Identification']['score_idx'] = 1\n",
    "protocol['Cluster Identification']['type'] = ['Cluster Identification']\n",
    "protocol['Cluster Identification']['length'] = 2\n",
    "protocol['Cluster Facet'] = {}\n",
    "protocol['Cluster Facet']['ad_idx'] = 1\n",
    "protocol['Cluster Facet']['score_idx'] = 2\n",
    "protocol['Cluster Facet']['type'] = ['Cluster Facet']\n",
    "protocol['Cluster Facet']['length'] = 3\n",
    "protocol['Cluster Aggregate'] = {}\n",
    "protocol['Cluster Aggregate']['ad_idx'] = 1\n",
    "protocol['Cluster Aggregate']['score_idx'] = 2\n",
    "protocol['Cluster Aggregate']['type'] = ['MODE', 'MIN', 'MAX', 'AVG']\n",
    "protocol['Cluster Aggregate']['length'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AvgP = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for qtype in protocol.keys():\n",
    "    AvgP[qtype] = {}\n",
    "\n",
    "    sub_data = Submissions[qtype]\n",
    "    gt_data = Ground_Truth[qtype]\n",
    "\n",
    "    try:\n",
    "        del submissions\n",
    "    except:\n",
    "        pass\n",
    "    for sub in sub_data:\n",
    "        if sub['type'] not in protocol[qtype]['type']:\n",
    "            print sub['type']\n",
    "            print \"TROUBLE\"\n",
    "            print qtype\n",
    "        submissions = clean_sub(sub['answer'], protocol[qtype]['ad_idx'], protocol[qtype]['score_idx'],\n",
    "                                protocol[qtype]['length'])\n",
    "        # Georgetown specific line:\n",
    "        q_id = sub['id']\n",
    "        if '-' in q_id:\n",
    "            q_id = q_id.split('-')[0]\n",
    "        gt = gt_data[q_id]\n",
    "        AvgP[qtype][q_id] = AvgPrecision(gt, submissions, protocol[qtype]['ad_idx'], protocol[qtype]['score_idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Question 94 from Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del AvgP['Cluster Aggregate']['94']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NewEval = {}\n",
    "\n",
    "for qtype in AvgP.keys():\n",
    "    NewEval[qtype] = {}\n",
    "    for quest in AvgP[qtype].keys():\n",
    "        NewEval[qtype][quest] = {}\n",
    "        NewEval[qtype][quest]['avg_precision'] = AvgP[qtype][quest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking into the Non-Uniform Aggregate Submission Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "73\n",
      "27\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Questions w/out Agg answers?\n",
    "no_answer_ids = []\n",
    "i = 0\n",
    "for q in Submissions['Cluster Aggregate']:\n",
    "    length = 0\n",
    "    try:\n",
    "        length = len(q['answer'][0])\n",
    "    except:\n",
    "        # If NO results given\n",
    "        no_answer_ids.append(q['id'])\n",
    "    if length == 3:\n",
    "        # If first result is an array of length 3\n",
    "        no_answer_ids.append(q['id'])\n",
    "    i += 1\n",
    "    \n",
    "answers_ids = []\n",
    "i = 0\n",
    "for q in Submissions['Cluster Aggregate']:\n",
    "    for entry in q['answer']:\n",
    "        if len(entry) != 3:\n",
    "            answers_ids.append(q['id'])\n",
    "    i += 1\n",
    "\n",
    "answers_ids = list(set(answers_ids))\n",
    "no_answer_ids = list(set(no_answer_ids))\n",
    "\n",
    "conflict1 = [x for x in no_answer_ids if x in answers_ids]\n",
    "print conflict1\n",
    "conflict2 = [x for x in answers_ids if x in no_answer_ids]\n",
    "print conflict2\n",
    "\n",
    "print len(no_answer_ids)\n",
    "print len(answers_ids)\n",
    "#for q in Submissions['Cluster Aggregate']:\n",
    "#    if q['id'] in no_answer_ids:\n",
    "#        try:\n",
    "#            print q['answer'][0]\n",
    "#        except:\n",
    "#            print q['answer']\n",
    "\n",
    "print \" \"\n",
    "#for q in Submissions['Cluster Aggregate']:\n",
    "#    if q['id'] in answers_ids:\n",
    "#        try:\n",
    "#            print q['answer'][0]\n",
    "#        except:\n",
    "#            print q['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Overall Aggregate Metric and Reformat Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for quest in Submissions['Cluster Aggregate']:\n",
    "    q_id = quest['id']\n",
    "    # Initiate metric at 0\n",
    "    metric = 0\n",
    "    # Skip submission for question 94\n",
    "    if q_id != '94':\n",
    "        # Only perform for questions determined to have agg answers\n",
    "        if q_id in answers_ids:\n",
    "            # Georgetown can supplym 2 aggregate answers.  Take first.\n",
    "            sub_answer = int(quest['answer'][0][0])\n",
    "            metric = AggMetric(agg_answ_gt[q_id]['agg_answer'], sub_answer, agg_answ_gt[q_id]['feature'])\n",
    "        NewEval['Cluster Aggregate'][q_id]['agg_metric'] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Point Fact Answer Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for quest in Submissions['Point Fact']:\n",
    "    q_id = quest['id']\n",
    "    pf_metric = PFMetric(pf_answ_gt[q_id], quest['answer'])\n",
    "    NewEval['Point Fact'][q_id]['pf_metric'] = pf_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Evaluation Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_file = 'Georgetown_' + extractions +'.json'\n",
    "with open(save_file, 'w') as outfile:\n",
    "    json.dump(NewEval, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirming Format of Point Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for quest in Submissions['Point Fact']:\n",
    "    for entry in quest['answer']:\n",
    "        score = 100\n",
    "        if len(entry) != 3:\n",
    "            print \"ENTRY LENGTH PROBLEM\"\n",
    "        if entry[2] > score:\n",
    "            print \"ORDER PROBLEM\"\n",
    "        score = entry[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
