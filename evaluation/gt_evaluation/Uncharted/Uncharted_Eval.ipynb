{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt_path = '../../../data/posted_GT/gt_answer_key/'\n",
    "# New\n",
    "agg_gt_answ_file = '../../../data/aggregate_gt/agg_answer_key.json'\n",
    "agg_metric_file = '../../../data/aggregate_gt/agg_metric_parameters.json'\n",
    "sub_path = '../../../data/team_submissions/Uncharted/GroundTruth/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_sub(sub, index_of_ad, index_of_score):\n",
    "    seen = []\n",
    "    uniqs = []\n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    for entry in sub:\n",
    "        # skip over aggregate single answer\n",
    "        if type(entry) == list:\n",
    "            if entry[index_of_score] > i:\n",
    "                print \"ORDERING PROBLEM\"\n",
    "                return None\n",
    "            i = entry[index_of_score]\n",
    "        #else:\n",
    "        #    print type(entry)\n",
    "    # de-dupe\n",
    "    for entry in sub:\n",
    "        # skip over aggregate single answer\n",
    "        if type(entry) == list:\n",
    "            if entry[index_of_ad] not in seen:\n",
    "                uniqs.append(entry)\n",
    "                seen.append(entry[index_of_ad])\n",
    "    # explicitly sort by score after de-duping\n",
    "    uniqs.sort(key=operator.itemgetter(index_of_score), reverse=True)\n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    for entry in uniqs:\n",
    "        if entry[index_of_score] > i:\n",
    "            print \"ORDERING PROBLEM\"\n",
    "            return None\n",
    "        i = entry[index_of_score]\n",
    "    return uniqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AvgPrecision(gt, sub, index_of_ad, index_of_score):\n",
    "    score = 100\n",
    "    seen = []\n",
    "    rel_docs = len(gt)\n",
    "    num_hits = 0\n",
    "    num_retrieved = 0\n",
    "    cumpre = 0\n",
    "    for entry in sub:\n",
    "        num_retrieved += 1\n",
    "        ad_id = entry[index_of_ad]\n",
    "        # excessive checking of dupes\n",
    "        if ad_id in seen:\n",
    "            print \"DUPLICATE\"\n",
    "            return None\n",
    "        seen.append(ad_id)\n",
    "        # excessive checking of order\n",
    "        if entry[index_of_score] > score:\n",
    "            print \"ORDER PROBLEM\"\n",
    "            return None\n",
    "        score = entry[index_of_score]\n",
    "        if ad_id in gt:\n",
    "            num_hits += 1\n",
    "            cumpre += num_hits/float(num_retrieved)\n",
    "            \n",
    "    return cumpre/rel_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AggMetric(gt_answ, sub_answ, feature):\n",
    "    scale_cutoff = agg_metric_params[feature]\n",
    "    diff = gt_answ - sub_answ\n",
    "    if numpy.absolute(diff) <= (gt_answ * scale_cutoff):\n",
    "        metric = 1\n",
    "    else:\n",
    "        metric = 0\n",
    "        \n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET EXTRACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extractions = 'Uncharted_Uncharted_only'\n",
    "#extractions = 'Uncharted_Uncharted_and_lattice'\n",
    "#extractions = 'Uncharted_CMU'\n",
    "\n",
    "if extractions == 'Uncharted_Uncharted_only':\n",
    "    sub_gt = sub_path + 'uncharted_extractions_GT.json'\n",
    "elif extractions == 'Uncharted_Uncharted_and_lattice':\n",
    "    sub_gt = sub_path + 'uncharted_lattice_extractions_GT.json'\n",
    "elif extractions == 'CMU':\n",
    "    sub_gt = sub_path + 'uncharted_lattice_extractions_CMU_clusters_GT.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(sub_gt, 'r')\n",
    "data = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "pf_sub = []\n",
    "id_sub = []\n",
    "facet_sub = []\n",
    "agg_sub = []\n",
    "\n",
    "for entry in data:\n",
    "    if entry['questionType'] == 'Point Fact':\n",
    "        pf_sub.append(entry)\n",
    "    elif entry['questionType'] == 'Cluster Identification':\n",
    "        id_sub.append(entry)\n",
    "    elif entry['questionType'] == 'Cluster Facet':\n",
    "        facet_sub.append(entry)\n",
    "    elif entry['questionType'] in ['MODE', 'MIN', 'MAX', 'AVG']:\n",
    "        agg_sub.append(entry)\n",
    "    else:\n",
    "        print entry['questionType']\n",
    "\n",
    "Submissions = {}\n",
    "Submissions['Point Fact'] = pf_sub\n",
    "Submissions['Cluster Identification'] = id_sub\n",
    "Submissions['Cluster Facet'] = facet_sub\n",
    "Submissions['Cluster Aggregate'] = agg_sub\n",
    "\n",
    "del pf_sub\n",
    "del id_sub\n",
    "del facet_sub\n",
    "del agg_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pf_path = gt_path + 'pf_quest2ads.json'\n",
    "f = open(pf_path, 'r')\n",
    "pf_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "id_path = gt_path + 'id_quest2ads.json'\n",
    "f = open(id_path, 'r')\n",
    "id_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "facet_path = gt_path + 'facet_quest2ads.json'\n",
    "f = open(facet_path, 'r')\n",
    "facet_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "agg_path = gt_path + 'agg_quest2ads.json'\n",
    "f = open(agg_path, 'r')\n",
    "agg_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "# New\n",
    "f = open(agg_gt_answ_file, 'r')\n",
    "agg_answ_gt = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "f = open(agg_metric_file, 'r')\n",
    "agg_metric_params = eval(f.read())\n",
    "f.close()\n",
    "\n",
    "Ground_Truth = {}\n",
    "Ground_Truth['Point Fact'] = pf_gt\n",
    "Ground_Truth['Cluster Identification'] = id_gt\n",
    "Ground_Truth['Cluster Facet'] = facet_gt\n",
    "Ground_Truth['Cluster Aggregate'] = agg_gt\n",
    "\n",
    "del pf_gt\n",
    "del id_gt\n",
    "del facet_gt\n",
    "del agg_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "protocol = {}\n",
    "protocol['Point Fact'] = {}\n",
    "protocol['Point Fact']['ad_idx'] = 1\n",
    "protocol['Point Fact']['score_idx'] = 2\n",
    "protocol['Point Fact']['questionType'] = ['Point Fact']\n",
    "protocol['Cluster Identification'] = {}\n",
    "protocol['Cluster Identification']['ad_idx'] = 0\n",
    "protocol['Cluster Identification']['score_idx'] = 1\n",
    "protocol['Cluster Identification']['questionType'] = ['Cluster Identification']\n",
    "protocol['Cluster Facet'] = {}\n",
    "protocol['Cluster Facet']['ad_idx'] = 1\n",
    "protocol['Cluster Facet']['score_idx'] = 2\n",
    "protocol['Cluster Facet']['questionType'] = ['Cluster Facet']\n",
    "protocol['Cluster Aggregate'] = {}\n",
    "protocol['Cluster Aggregate']['ad_idx'] = 1\n",
    "protocol['Cluster Aggregate']['score_idx'] = 2\n",
    "protocol['Cluster Aggregate']['questionType'] = ['MODE', 'MIN', 'MAX', 'AVG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AvgP = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for qtype in protocol.keys():\n",
    "    AvgP[qtype] = {}\n",
    "\n",
    "    sub_data = Submissions[qtype]\n",
    "    gt_data = Ground_Truth[qtype]\n",
    "\n",
    "    try:\n",
    "        del submissions\n",
    "    except:\n",
    "        pass\n",
    "    for sub in sub_data:\n",
    "        if sub['questionType'] not in protocol[qtype]['questionType']:\n",
    "            print sub['questionType']\n",
    "            print \"TROUBLE\"\n",
    "            print qtype\n",
    "        submissions = clean_sub(sub['answers'], protocol[qtype]['ad_idx'], protocol[qtype]['score_idx'])\n",
    "        q_id = sub['question_id']\n",
    "        gt = gt_data[q_id]\n",
    "        AvgP[qtype][q_id] = AvgPrecision(gt, submissions, protocol[qtype]['ad_idx'], protocol[qtype]['score_idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Question 94 from Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del AvgP['Cluster Aggregate']['94']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NewEval = {}\n",
    "\n",
    "for qtype in AvgP.keys():\n",
    "    NewEval[qtype] = {}\n",
    "    for quest in AvgP[qtype].keys():\n",
    "        NewEval[qtype][quest] = {}\n",
    "        NewEval[qtype][quest]['avg_precision'] = AvgP[qtype][quest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Overall Aggregate Metric and Reformat Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for quest in Submissions['Cluster Aggregate']:\n",
    "    q_id = quest['question_id']\n",
    "    # Initiate metric at 0\n",
    "    metric = 0\n",
    "    # Skip submission for question 94\n",
    "    if q_id != '94':\n",
    "        if len(quest['answers']) > 0:\n",
    "            sub_answer = quest['answers'][0]\n",
    "            metric = AggMetric(agg_answ_gt[q_id]['agg_answer'], sub_answer, agg_answ_gt[q_id]['feature'])\n",
    "        NewEval['Cluster Aggregate'][q_id]['agg_metric'] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Point Fact Extraction Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Evaluation Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_file = extractions +'.json'\n",
    "with open(save_file, 'w') as outfile:\n",
    "    json.dump(NewEval, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
