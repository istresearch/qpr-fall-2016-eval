{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool_depth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sub(raw_sub, index_of_ad, index_of_score, expected_length):\n",
    "    seen = []\n",
    "    uniqs = []\n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    # Pull out entries not of expected length\n",
    "    sub = []\n",
    "    for x in raw_sub:\n",
    "        if type(x) == list:\n",
    "            if len(x) == expected_length:\n",
    "                sub.append(x)\n",
    "    sub.sort(key=operator.itemgetter(index_of_score), reverse=True)\n",
    "    \n",
    "    for entry in sub:\n",
    "        if len(entry) == expected_length:\n",
    "            if entry[index_of_score] > i:\n",
    "                print \"ORDERING PROBLEM\"\n",
    "                return None\n",
    "            i = entry[index_of_score]\n",
    "            #else:\n",
    "            #    print type(entry)\n",
    "            \n",
    "    # de-dupe\n",
    "    for entry in sub:\n",
    "        if len(entry) == expected_length:\n",
    "            if entry[index_of_ad] not in seen:\n",
    "                uniqs.append(entry)\n",
    "                seen.append(entry[index_of_ad])\n",
    "                \n",
    "    # explicitly sort by score after de-duping\n",
    "    uniqs.sort(key=operator.itemgetter(index_of_score), reverse=True)\n",
    "    \n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    for entry in uniqs:\n",
    "        if entry[index_of_score] > i:\n",
    "            print \"ORDERING PROBLEM\"\n",
    "            return None\n",
    "        i = entry[index_of_score]\n",
    "        \n",
    "    return uniqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_subs(team, entry, id_pos, score_pos, ans_length):\n",
    "    submissions = clean_sub(entry[configs[team]['afield']], id_pos, score_pos, ans_length)\n",
    "    # Reconfirming no dupes\n",
    "    if len(submissions) != len(list(set(tuple(i) for i in submissions))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "    # Now that submissions are sorted, only take top N\n",
    "    if len(submissions) > pool_depth:\n",
    "        submissions = submissions[:pool_depth]\n",
    "        #print \"POOL DEPTH REACHED\"\n",
    "    \n",
    "    return submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note which questions are to be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions_file =  '../annotation_prep/dd_clustering/FIRST_ROUND_chosen_questions.json'\n",
    "\n",
    "with open(questions_file, 'r') as f:\n",
    "    questions = eval(f.read())\n",
    "\n",
    "question_ids = {}\n",
    "for qtype in questions['NYU'].keys():\n",
    "    question_ids[qtype] = questions['NYU'][qtype].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Submissions From Each Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "configs = {}\n",
    "configs['Georgetown'] = {}\n",
    "configs['Georgetown']['qfield'] = 'id'\n",
    "configs['Georgetown']['afield'] = 'answer'\n",
    "configs['Georgetown']['Cluster Identification'] = {}\n",
    "configs['Georgetown']['Cluster Identification']['id_pos'] = 0\n",
    "configs['Georgetown']['Cluster Identification']['score_pos'] = 1\n",
    "configs['Georgetown']['Cluster Identification']['exp_length'] = 2\n",
    "configs['Georgetown']['Cluster Facet'] = {}\n",
    "configs['Georgetown']['Cluster Facet']['id_pos'] = 1\n",
    "configs['Georgetown']['Cluster Facet']['score_pos'] = 2\n",
    "configs['Georgetown']['Cluster Facet']['exp_length'] = 3\n",
    "configs['Georgetown']['Cluster Aggregate'] = {}\n",
    "configs['Georgetown']['Cluster Aggregate']['id_pos'] = 1\n",
    "configs['Georgetown']['Cluster Aggregate']['score_pos'] = 2\n",
    "configs['Georgetown']['Cluster Aggregate']['exp_length'] = 3\n",
    "\n",
    "configs['ISI'] = {}\n",
    "configs['ISI']['qfield'] = 'question_id'\n",
    "configs['ISI']['afield'] = 'answer'\n",
    "configs['ISI']['Cluster Identification'] = {}\n",
    "configs['ISI']['Cluster Identification']['id_pos'] = 0\n",
    "configs['ISI']['Cluster Identification']['score_pos'] = 1\n",
    "configs['ISI']['Cluster Identification']['exp_length'] = 2\n",
    "configs['ISI']['Cluster Facet'] = {}\n",
    "configs['ISI']['Cluster Facet']['id_pos'] = 1\n",
    "configs['ISI']['Cluster Facet']['score_pos'] = 2\n",
    "configs['ISI']['Cluster Facet']['exp_length'] = 3\n",
    "configs['ISI']['Cluster Aggregate'] = {}\n",
    "configs['ISI']['Cluster Aggregate']['id_pos'] = 1\n",
    "configs['ISI']['Cluster Aggregate']['score_pos'] = 2\n",
    "configs['ISI']['Cluster Aggregate']['exp_length'] = 3\n",
    "\n",
    "configs['Uncharted'] = {}\n",
    "configs['Uncharted']['qfield'] = 'question_id'\n",
    "configs['Uncharted']['afield'] = 'answers'\n",
    "configs['Uncharted']['Cluster Identification'] = {}\n",
    "configs['Uncharted']['Cluster Identification']['id_pos'] = 0\n",
    "configs['Uncharted']['Cluster Identification']['score_pos'] = 1\n",
    "configs['Uncharted']['Cluster Identification']['exp_length'] = 2\n",
    "configs['Uncharted']['Cluster Facet'] = {}\n",
    "configs['Uncharted']['Cluster Facet']['id_pos'] = 1\n",
    "configs['Uncharted']['Cluster Facet']['score_pos'] = 2\n",
    "configs['Uncharted']['Cluster Facet']['exp_length'] = 3\n",
    "configs['Uncharted']['Cluster Aggregate'] = {}\n",
    "configs['Uncharted']['Cluster Aggregate']['id_pos'] = 1\n",
    "configs['Uncharted']['Cluster Aggregate']['score_pos'] = 2\n",
    "configs['Uncharted']['Cluster Aggregate']['exp_length'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_file_paths = {}\n",
    "submission_file_paths['Georgetown'] = {}\n",
    "submission_file_paths['Georgetown']['NYU'] = {}\n",
    "submission_file_paths['Georgetown']['NYU']['Cluster Identification'] = 'Georgetown/DomainDiscovery/NYU_CI.json'\n",
    "submission_file_paths['Georgetown']['NYU']['Cluster Facet'] = 'Georgetown/DomainDiscovery/NYU_CF.json'\n",
    "submission_file_paths['Georgetown']['NYU']['Cluster Aggregate'] = 'Georgetown/DomainDiscovery/NYU_aggregate.json'\n",
    "submission_file_paths['Georgetown']['HG'] = {}\n",
    "submission_file_paths['Georgetown']['HG']['Cluster Identification'] = ('Georgetown/DomainDiscovery/'\n",
    "                                                                       'Georgetown_Submission/HG/'\n",
    "                                                                       'HG_Cluster_Identification.json')\n",
    "submission_file_paths['Georgetown']['HG']['Cluster Facet'] = ('Georgetown/DomainDiscovery/'\n",
    "                                                              'Georgetown_Submission/HG/'\n",
    "                                                              'HG_Cluster_Facet.json')\n",
    "submission_file_paths['Georgetown']['HG']['Cluster Aggregate'] = ('Georgetown/DomainDiscovery/'\n",
    "                                                                  'Georgetown_Submission/HG/'\n",
    "                                                                  'HG_Aggregate.json')\n",
    "submission_file_paths['Georgetown']['JPL'] = {}\n",
    "submission_file_paths['Georgetown']['JPL']['Cluster Identification'] = ('Georgetown/DomainDiscovery/'\n",
    "                                                                       'Georgetown_Submission/JPL/'\n",
    "                                                                       'JPL_Cluster_Identification.json')\n",
    "submission_file_paths['Georgetown']['JPL']['Cluster Facet'] = ('Georgetown/DomainDiscovery/'\n",
    "                                                              'Georgetown_Submission/JPL/'\n",
    "                                                              'JPL_Cluster_Facet.json')\n",
    "submission_file_paths['Georgetown']['JPL']['Cluster Aggregate'] = ('Georgetown/DomainDiscovery/'\n",
    "                                                                  'Georgetown_Submission/JPL/'\n",
    "                                                                  'JPL_Aggregate.json')\n",
    "\n",
    "submission_file_paths['ISI'] = {}\n",
    "submission_file_paths['ISI']['NYU'] = {}\n",
    "submission_file_paths['ISI']['NYU']['Cluster Identification'] = ('ISI/DomainDiscovery/isi-nyu-answers-dig-extractions/'\n",
    "                                                                 'properly_formatted_submissions/'\n",
    "                                                                 'formatted_post_cluster_identification'\n",
    "                                                                 '-parsed_fixed_all_answers.json')\n",
    "submission_file_paths['ISI']['NYU']['Cluster Facet'] = ('ISI/DomainDiscovery/isi-nyu-answers-dig-extractions/'\n",
    "                                                        'properly_formatted_submissions/'\n",
    "                                                        'formatted_post_cluster_facet'\n",
    "                                                        '_parsed_fixed_all_answers.json')\n",
    "submission_file_paths['ISI']['NYU']['Cluster Aggregate'] = ('ISI/DomainDiscovery/isi-nyu-answers-dig-extractions/'\n",
    "                                                            'properly_formatted_submissions/'\n",
    "                                                            'formatted_post_aggregate'\n",
    "                                                            '_parsed_fixed_all_answers.json')\n",
    "submission_file_paths['ISI']['HG'] = {}\n",
    "submission_file_paths['ISI']['HG']['Cluster Identification'] = ('ISI/DomainDiscovery/hg_all_asnwers/'\n",
    "                                                                 'properly_formatted_submissions/'\n",
    "                                                                 'formatted_post_cluster_identification'\n",
    "                                                                 '-parsed_fixed_all_answers.json')\n",
    "submission_file_paths['ISI']['HG']['Cluster Facet'] = ('ISI/DomainDiscovery/hg_all_asnwers/'\n",
    "                                                        'properly_formatted_submissions/'\n",
    "                                                        'formatted_post_cluster_facet'\n",
    "                                                        '_parsed_fixed_all_answers.json')\n",
    "submission_file_paths['ISI']['HG']['Cluster Aggregate'] = ('ISI/DomainDiscovery/hg_all_asnwers/'\n",
    "                                                            'properly_formatted_submissions/'\n",
    "                                                            'formatted_post_aggregate'\n",
    "                                                            '_parsed_fixed_all_answers.json')\n",
    "submission_file_paths['ISI']['JPL'] = {}\n",
    "submission_file_paths['ISI']['JPL']['Cluster Identification'] = ('ISI/DomainDiscovery/jpl_answers_isi/'\n",
    "                                                                 'properly_formatted_submissions/'\n",
    "                                                                 'formatted_cluster-identification-queries-parsed_all_answers.json')\n",
    "submission_file_paths['ISI']['JPL']['Cluster Facet'] = ('ISI/DomainDiscovery/jpl_answers_isi/'\n",
    "                                                        'properly_formatted_submissions/'\n",
    "                                                        'formatted_cluster-facet-queries-parsed_all_answers.json')\n",
    "submission_file_paths['ISI']['JPL']['Cluster Aggregate'] = ('ISI/DomainDiscovery/jpl_answers_isi/'\n",
    "                                                            'properly_formatted_submissions/'\n",
    "                                                            'formatted_aggregate-queries-parsed_all_answers.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "ISI\n",
      " \n",
      "JPL\n",
      "Cluster Aggregate\n",
      "Cluster Facet\n",
      "Cluster Identification\n",
      "HG\n",
      "Cluster Aggregate\n",
      "Cluster Facet\n",
      "Cluster Identification\n",
      "NYU\n",
      "Cluster Aggregate\n",
      "Cluster Facet\n",
      "Cluster Identification\n"
     ]
    }
   ],
   "source": [
    "answers = {}\n",
    "\n",
    "for team in submission_file_paths.keys():\n",
    "#for team in ['ISI']:\n",
    "    print ' '\n",
    "    print team.upper()\n",
    "    print ' '\n",
    "    answers[team] = {}\n",
    "    for dataset in submission_file_paths[team].keys():\n",
    "        print dataset\n",
    "        answers[team][dataset] = {}\n",
    "        for qtype in submission_file_paths[team][dataset].keys():\n",
    "            print qtype\n",
    "            answers[team][dataset][qtype] = {}\n",
    "            # Load team's submissions for that qtype and dataset\n",
    "            file_path = submission_file_paths[team][dataset][qtype]\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = eval(f.read())\n",
    "                \n",
    "            for entry in data:\n",
    "                # If the question id is to be evaluated\n",
    "                if team == 'ISI':\n",
    "                    qid = entry[configs[team]['qfield']] .split('-')[0]\n",
    "                else:\n",
    "                    qid = entry[configs[team]['qfield']]\n",
    "                if qid in question_ids[qtype]:\n",
    "                    # Some questions have no answers\n",
    "                    if len(entry[configs[team]['afield']]) > 0:\n",
    "                        answers[team][dataset][qtype][qid] = {}\n",
    "                        submissions = get_subs(team, entry, configs[team][qtype]['id_pos'],\n",
    "                                              configs[team][qtype]['score_pos'], configs[team][qtype]['exp_length'])\n",
    "                    else:\n",
    "                        submissions = []\n",
    "                    \n",
    "                    answers[team][dataset][qtype][qid]['submissions'] = submissions\n",
    "                    if qtype == 'Cluster Aggregate':\n",
    "                        if type(entry[configs[team]['afield']][0]) != list:\n",
    "                            agg_answ = entry[configs[team]['afield']][0]\n",
    "                        # It could be a list of multiple submissions\n",
    "                        elif len(entry[configs[team]['afield']][0]) < configs[team][qtype]['exp_length']:\n",
    "                            agg_answ = entry[configs[team]['afield']][0]\n",
    "                        else:\n",
    "                            agg_answ = \"NA\"\n",
    "                            # Confirmed \"NO AGGS\" for Georgetown, \n",
    "                            print \"NO AGG FOUND FOR QUESTION {0}\".format(entry[configs[team]['qfield']])\n",
    "                        answers[team][dataset][qtype][qid]['agg_answ'] = agg_answ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Relevant Ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_file = 'collected_cluster_answers_REWRITE.json'\n",
    "# Will re-name manually to avoid overwriting\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(answers, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_round_file = 'FIRST_ROUND_seeds2cluster_ads.json'\n",
    "second_round_file = 'FIRST_ROUND_seeds2cluster_ads.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(first_round_file, 'r') as f:\n",
    "    first_round = eval(f.read())\n",
    "\n",
    "with open(second_round_file, 'r') as f:\n",
    "    second_round = eval(f.read())\n",
    "    \n",
    "new_seeds2cluster_ad = {}\n",
    "\n",
    "for seed in first_round.keys():\n",
    "    temp1 = first_round[seed]\n",
    "    temp2 = second_round[seed]\n",
    "    # List comprehension can cause backwards updates\n",
    "    new_temp = [x for x in temp1]\n",
    "    new_temp.extend(temp2)\n",
    "    new_temp = list(set(new_temp))\n",
    "    \n",
    "    # There shouldn't be any dupes\n",
    "    if len(new_temp) != len(temp1) + len(temp2):\n",
    "        print 'TROUBLE'\n",
    "    \n",
    "    new_seeds2cluster_ad[seed] = new_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute Relevant Ads to a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/Rich/repos/qpr-fall-2016-eval/data/team_submissions'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
