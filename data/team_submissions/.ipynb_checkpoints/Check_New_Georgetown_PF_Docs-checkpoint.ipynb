{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pool still 100 for PF DD:\n",
    "https://memexproxy.com/wiki/display/MPM/Post-hoc+annotation+protocol\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool_depth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sub(raw_sub, index_of_ad, index_of_score, expected_length):\n",
    "    seen = []\n",
    "    uniqs = []\n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    # Pull out entries not of expected length\n",
    "    sub = []\n",
    "    for x in raw_sub:\n",
    "        if type(x) == list:\n",
    "            if len(x) == expected_length:\n",
    "                sub.append(x)\n",
    "    sub.sort(key=operator.itemgetter(index_of_score), reverse=True)\n",
    "    \n",
    "    for entry in sub:\n",
    "        if len(entry) == expected_length:\n",
    "            if entry[index_of_score] > i:\n",
    "                print \"ORDERING PROBLEM\"\n",
    "                return None\n",
    "            i = entry[index_of_score]\n",
    "            #else:\n",
    "            #    print type(entry)\n",
    "            \n",
    "    # de-dupe\n",
    "    for entry in sub:\n",
    "        if len(entry) == expected_length:\n",
    "            if entry[index_of_ad] not in seen:\n",
    "                uniqs.append(entry)\n",
    "                seen.append(entry[index_of_ad])\n",
    "                \n",
    "    # explicitly sort by score after de-duping\n",
    "    uniqs.sort(key=operator.itemgetter(index_of_score), reverse=True)\n",
    "    \n",
    "    # explicitly check ordering\n",
    "    i = 100\n",
    "    for entry in uniqs:\n",
    "        if entry[index_of_score] > i:\n",
    "            print \"ORDERING PROBLEM\"\n",
    "            return None\n",
    "        i = entry[index_of_score]\n",
    "        \n",
    "    return uniqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_subs(team, entry, id_pos, score_pos, ans_length):\n",
    "    submissions = clean_sub(entry[configs[team]['afield']], id_pos, score_pos, ans_length)\n",
    "    # Reconfirming no dupes\n",
    "    if len(submissions) != len(list(set(tuple(i) for i in submissions))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "    # Now that submissions are sorted, only take top N\n",
    "    if len(submissions) > pool_depth:\n",
    "        submissions = submissions[:pool_depth]\n",
    "        #print \"POOL DEPTH REACHED\"\n",
    "    \n",
    "    return submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note which questions are to be evaluated (From Marti)\n",
    "\n",
    "https://istresearch.slack.com/archives/memex-qpr/p1486669437002437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question_ids = {}\n",
    "question_ids['Point Fact'] = ['1647', '392', '1707', '217', '510', '799',\n",
    "                '363', '1597', '1180', '1159', '1035',\n",
    "                '1038', '2304', '1339', '284']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Submissions From Each Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "configs = {}\n",
    "configs['Georgetown'] = {}\n",
    "configs['Georgetown']['qfield'] = 'id'\n",
    "configs['Georgetown']['afield'] = 'answer'\n",
    "configs['Georgetown']['Point Fact'] = {}\n",
    "configs['Georgetown']['Point Fact']['id_pos'] = 1\n",
    "configs['Georgetown']['Point Fact']['score_pos'] = 2\n",
    "configs['Georgetown']['Point Fact']['exp_length'] = 3\n",
    "\n",
    "configs['ISI'] = {}\n",
    "configs['ISI']['qfield'] = 'question_id'\n",
    "configs['ISI']['afield'] = 'answer'\n",
    "configs['ISI']['Point Fact'] = {}\n",
    "configs['ISI']['Point Fact']['id_pos'] = 1\n",
    "configs['ISI']['Point Fact']['score_pos'] = 2\n",
    "configs['ISI']['Point Fact']['exp_length'] = 3\n",
    "\n",
    "configs['Uncharted'] = {}\n",
    "configs['Uncharted']['qfield'] = 'question_id'\n",
    "configs['Uncharted']['afield'] = 'answers'\n",
    "configs['Uncharted']['Point Fact'] = {}\n",
    "configs['Uncharted']['Point Fact']['id_pos'] = 1\n",
    "configs['Uncharted']['Point Fact']['score_pos'] = 2\n",
    "configs['Uncharted']['Point Fact']['exp_length'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_file_paths = {}\n",
    "submission_file_paths['Georgetown'] = {}\n",
    "submission_file_paths['Georgetown']['HG'] = {}\n",
    "submission_file_paths['Georgetown']['HG']['Point Fact'] = ('Georgetown/DomainDiscovery/'\n",
    "                                                                       'Georgetown_Submission_2_8/HG/'\n",
    "                                                                       'HG_Point_Fact_2_9.json')\n",
    "submission_file_paths['Georgetown']['JPL'] = {}\n",
    "submission_file_paths['Georgetown']['JPL']['Point Fact'] = ('Georgetown/DomainDiscovery/'\n",
    "                                                                       'Georgetown_Submission_2_8/JPL/'\n",
    "                                                                       'JPL_Point_Fact_2_9.json')\n",
    "\n",
    "submission_file_paths['ISI'] = {}\n",
    "submission_file_paths['ISI']['HG'] = {}\n",
    "submission_file_paths['ISI']['HG']['Point Fact'] = ('ISI/DomainDiscovery/hg_all_asnwers/'\n",
    "                                                                 'properly_formatted_submissions/'\n",
    "                                                                 'formatted_post_point_fact_parsed_fixed_all_answers.json')\n",
    "submission_file_paths['ISI']['JPL'] = {}\n",
    "submission_file_paths['ISI']['JPL']['Point Fact'] = ('ISI/DomainDiscovery/jpl_answers_isi/'\n",
    "                                                                 'properly_formatted_submissions/'\n",
    "                                                                 'formatted_PF-queries-parsed_all_answers.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "GEORGETOWN\n",
      " \n",
      "HG\n",
      "Point Fact\n",
      "JPL\n",
      "Point Fact\n",
      " \n",
      "ISI\n",
      " \n",
      "HG\n",
      "Point Fact\n",
      "JPL\n",
      "Point Fact\n"
     ]
    }
   ],
   "source": [
    "answers = {}\n",
    "\n",
    "debug_data = {}\n",
    "for team in submission_file_paths.keys():\n",
    "#for team in ['Georgetown']:\n",
    "    debug_data[team] = {}\n",
    "    print ' '\n",
    "    print team.upper()\n",
    "    print ' '\n",
    "    answers[team] = {}\n",
    "    for dataset in submission_file_paths[team].keys():\n",
    "        debug_data[team][dataset] = {}\n",
    "        print dataset\n",
    "        answers[team][dataset] = {}\n",
    "        for qtype in submission_file_paths[team][dataset].keys():\n",
    "            \n",
    "            # Account for disparate format b/w Georgetown HG and JPL submissions\n",
    "            id_pos = configs[team][qtype]['id_pos']\n",
    "            score_pos = configs[team][qtype]['score_pos']\n",
    "            exp_length = configs[team][qtype]['exp_length']\n",
    "            if team == 'Georgetown':\n",
    "                if qtype == 'Cluster Aggregate':\n",
    "                    if dataset == 'JPL':\n",
    "                        id_pos = 0\n",
    "                        score_pos = 1\n",
    "                        exp_length = 2\n",
    "            \n",
    "            print qtype\n",
    "            answers[team][dataset][qtype] = {}\n",
    "            # Load team's submissions for that qtype and dataset\n",
    "            file_path = submission_file_paths[team][dataset][qtype]\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = eval(f.read())\n",
    "            \n",
    "            debug_data[team][dataset][qtype] = data\n",
    "            for entry in data:\n",
    "                # If the question id is to be evaluated\n",
    "                if team == 'ISI':\n",
    "                    qid = entry[configs[team]['qfield']] .split('-')[0]\n",
    "                else:\n",
    "                    qid = entry[configs[team]['qfield']]\n",
    "                if qid in question_ids[qtype]:\n",
    "                    answers[team][dataset][qtype][qid] = {}\n",
    "                    # Some questions have no answers\n",
    "                    if len(entry[configs[team]['afield']]) > 0:\n",
    "                        submissions = get_subs(team, entry, id_pos, score_pos, exp_length)\n",
    "                    else:\n",
    "                        submissions = []\n",
    "                    answers[team][dataset][qtype][qid]['submissions'] = submissions\n",
    "                    \n",
    "                    if qtype == 'Cluster Aggregate':\n",
    "                        # Some questions have no answers\n",
    "                        if len(entry[configs[team]['afield']]) > 0:\n",
    "                            if type(entry[configs[team]['afield']][0]) != list:\n",
    "                                agg_answ = entry[configs[team]['afield']][0]\n",
    "                            # It could be a list of multiple submissions\n",
    "                            elif len(entry[configs[team]['afield']][0]) < exp_length:\n",
    "                                agg_answ = entry[configs[team]['afield']][0]\n",
    "                            else:\n",
    "                                agg_answ = \"NA\"\n",
    "                        else:\n",
    "                            agg_answ = \"NA\"\n",
    "                            # Confirmed \"NO AGGS\" for Georgetown, and low response for JPL\n",
    "                            print \"NO AGG FOUND FOR QUESTION {0}\".format(entry[configs[team]['qfield']])\n",
    "                        answers[team][dataset][qtype][qid]['agg_answ'] = agg_answ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Need to get Uncharted's a different way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uncharted_files = {}\n",
    "uncharted_files['HG'] = 'Uncharted/DomainDiscovery/uncharted_HG_DD.json'\n",
    "uncharted_files['JPL'] = 'Uncharted/DomainDiscovery/uncharted_JPL_DD.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qtype = 'Point Fact'\n",
    "team = 'Uncharted'\n",
    "answers[team] = {}\n",
    "\n",
    "for dataset in uncharted_files.keys():\n",
    "    answers[team][dataset] = {}\n",
    "    answers[team][dataset][qtype] = {}\n",
    "    \n",
    "    file_path = uncharted_files[dataset]\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = eval(f.read())\n",
    "        \n",
    "    # Account for disparate format b/w Georgetown HG and JPL submissions\n",
    "    id_pos = configs[team][qtype]['id_pos']\n",
    "    score_pos = configs[team][qtype]['score_pos']\n",
    "    exp_length = configs[team][qtype]['exp_length']\n",
    "    \n",
    "    for entry in data:\n",
    "        if entry['questionType'] == qtype:\n",
    "            qid = entry[configs[team]['qfield']]\n",
    "            if qid in question_ids[qtype]:\n",
    "                answers[team][dataset][qtype][qid] = {}\n",
    "                # Some questions have no answers\n",
    "                if len(entry[configs[team]['afield']]) > 0:\n",
    "                    submissions = get_subs(team, entry, id_pos, score_pos, exp_length)\n",
    "                else:\n",
    "                    submissions = []\n",
    "                answers[team][dataset][qtype][qid]['submissions'] = submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate list of IDs that should already have been submitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qtype = 'Point Fact'\n",
    "\n",
    "prev_ids = {}\n",
    "prev_ids['HG'] = []\n",
    "prev_ids['JPL'] = []\n",
    "\n",
    "for team in ['ISI', 'Uncharted']:\n",
    "    for dataset in answers[team].keys():\n",
    "        for quest in answers[team][dataset][qtype].keys():\n",
    "            for entry in answers[team][dataset][qtype][quest]['submissions']:\n",
    "                prev_ids[dataset].append(entry[configs[team][qtype]['id_pos']])\n",
    "                \n",
    "uniq_prev_ids = {}\n",
    "uniq_prev_ids['HG'] = list(set(prev_ids['HG']))\n",
    "uniq_prev_ids['JPL'] = list(set(prev_ids['JPL']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find IDs unique to Georgetown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qtype = 'Point Fact'\n",
    "\n",
    "georgetown_ids = {}\n",
    "georgetown_ids['HG'] = []\n",
    "georgetown_ids['JPL'] = []\n",
    "\n",
    "for team in ['Georgetown']:\n",
    "    for dataset in answers[team].keys():\n",
    "        for quest in answers[team][dataset][qtype].keys():\n",
    "            for entry in answers[team][dataset][qtype][quest]['submissions']:\n",
    "                georgetown_ids[dataset].append(entry[configs[team][qtype]['id_pos']])\n",
    "                \n",
    "uniq_georgetown_ids = {}\n",
    "uniq_georgetown_ids['HG'] = list(set(georgetown_ids['HG']))\n",
    "uniq_georgetown_ids['JPL'] = list(set(georgetown_ids['JPL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_ids = {}\n",
    "new_ids['HG'] = []\n",
    "new_ids['JPL'] = []\n",
    "\n",
    "for dataset in uniq_georgetown_ids.keys():\n",
    "    new = [x for x in uniq_georgetown_ids[dataset] if x not in uniq_prev_ids[dataset]]\n",
    "    new_ids[dataset] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503\n",
      "630\n"
     ]
    }
   ],
   "source": [
    "print len(new_ids['HG'])\n",
    "\n",
    "print len(new_ids['JPL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute Relevant Ads to a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
