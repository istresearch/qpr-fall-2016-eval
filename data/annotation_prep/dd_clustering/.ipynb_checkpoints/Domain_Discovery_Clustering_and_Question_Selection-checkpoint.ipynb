{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pooling Depth for Each Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool_depth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Number of Questions to be Evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_depth = {\n",
    "    'Cluster Identification': 7,\n",
    "    'Cluster Facet': 7,\n",
    "    'Cluster Aggregate': 15,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_path = '../../team_submissions/'\n",
    "questions_path = '../../../questions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_seed(question):\n",
    "    lines = question['SPARQL'][0].split('\\n')\n",
    "    seed = str(lines[4].split(' ')[1]).strip(\"'\")\n",
    "        \n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_pool(pooling_level, id_pos, score_pos, ans_length, id_length, responses):\n",
    "    # Explicitly check for score ordering\n",
    "    score = 100\n",
    "    count = 0\n",
    "    seen = []\n",
    "    for answer in responses:\n",
    "        # Non-Array, leading elements may happen to have correct\n",
    "        # length\n",
    "        if type(answer) == list:\n",
    "            # Assume elements not matching expected length\n",
    "            # are aggregate answers\n",
    "            if len(answer) == ans_length:\n",
    "                # Confirming doc id is where we expect\n",
    "                if len(answer[id_pos]) != id_length:\n",
    "                    print \"NONSTANDARD DOC ID DETECTED\"\n",
    "                    print answer\n",
    "                    break\n",
    "                if answer[score_pos] > score:\n",
    "                    print (score, answer[score_pos])\n",
    "                    print \"RANK ORDER ISSUE\"\n",
    "                    break\n",
    "                score = answer[score_pos]\n",
    "                if answer[id_pos] not in seen:\n",
    "                    # Found another unique doc id\n",
    "                    seen.append(answer[id_pos])\n",
    "                    count += 1\n",
    "                    if count == pooling_level:\n",
    "                        # Found top N docs\n",
    "                        # Confirm no duplicates\n",
    "                        len1 = len(seen)\n",
    "                        uniq_seen = list(set(seen))\n",
    "                        if len1 != len(uniq_seen):\n",
    "                            print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "                            break\n",
    "                        return uniq_seen\n",
    "    # Or even if you don't get N uniq\n",
    "    uniq_seen = list(set(seen))\n",
    "    return uniq_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Seed for Every Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seeds = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seeds['Cluster Identification'] = {}\n",
    "\n",
    "file_path = questions_path + 'post_cluster_identification.json'\n",
    "f = open(file_path, 'r')\n",
    "for line in f:\n",
    "    temp = json.loads(line)\n",
    "    seed = extract_seed(temp)\n",
    "    seeds['Cluster Identification'][temp['id']] = seed\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seeds['Cluster Facet'] = {}\n",
    "\n",
    "file_path = questions_path + 'post_cluster_facet.json'\n",
    "f = open(file_path, 'r')\n",
    "for line in f:\n",
    "    temp = json.loads(line)\n",
    "    seed = extract_seed(temp)\n",
    "    seeds['Cluster Facet'][temp['id']] = seed\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seeds['Cluster Aggregate'] = {}\n",
    "\n",
    "file_path = questions_path + 'post_aggregate_V2.json'\n",
    "f = open(file_path, 'r')\n",
    "for line in f:\n",
    "    temp = json.loads(line)\n",
    "    seed = extract_seed(temp)\n",
    "    seeds['Cluster Aggregate'][temp['id']] = seed\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Georgetown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['NYU'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown Cluster Identification\n",
    "NOTE: Response missing for Question #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['NYU']['Georgetown'] = {}\n",
    "clus_type = 'Cluster Identification'\n",
    "answers['NYU']['Georgetown'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = submission_path + 'Georgetown/DomainDiscovery/NYU_CI.json'\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 0\n",
    "score_pos = 1\n",
    "ans_length = 2 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    answers['NYU']['Georgetown'][clus_type][entry['id']] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Facet'\n",
    "answers['NYU']['Georgetown'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = submission_path + 'Georgetown/DomainDiscovery/NYU_CF.json'\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    answers['NYU']['Georgetown'][clus_type][entry['id']] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Aggregate'\n",
    "answers['NYU']['Georgetown'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = submission_path + 'Georgetown/DomainDiscovery/NYU_aggregate.json'\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    # Cluster Aggregate Question 94 was Removed\n",
    "    if entry['id'] != '94':\n",
    "        top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "        # Reconfirming no dupes\n",
    "        if len(top_ids) != len(list(set(top_ids))):\n",
    "            print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "            break\n",
    "    answers['NYU']['Georgetown'][clus_type][entry['id']] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['NYU']['ISI'] = {}\n",
    "clus_type = 'Cluster Identification'\n",
    "answers['NYU']['ISI'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'isi-nyu-answers-dig-extractions/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_post_cluster_identification'\n",
    "                               '-parsed_fixed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    answers['NYU']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Facet'\n",
    "answers['NYU']['ISI'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'isi-nyu-answers-dig-extractions/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_post_cluster_facet'\n",
    "                               '_parsed_fixed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    answers['NYU']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Aggregate'\n",
    "answers['NYU']['ISI'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'isi-nyu-answers-dig-extractions/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_post_aggregate'\n",
    "                               '_parsed_fixed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    answers['NYU']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncharted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncharted NYU-Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['NYU']['Uncharted'] = {}\n",
    "clus_type = 'Cluster Identification'\n",
    "answers['NYU']['Uncharted'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_NYU_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "data = []\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == clus_type:\n",
    "        data.append(entry)\n",
    "        \n",
    "id_pos = 0\n",
    "score_pos = 1\n",
    "ans_length = 2 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    # Not all entries have 'answers'\n",
    "    if 'answers' in entry.keys():\n",
    "        top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                           ans_length, id_length, entry['answers'])\n",
    "        # Reconfirming no dupes\n",
    "        if len(top_ids) != len(list(set(top_ids))):\n",
    "            print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "            break\n",
    "    answers['NYU']['Uncharted'][clus_type][entry['question_id']] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncharted NYU-Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Facet'\n",
    "answers['NYU']['Uncharted'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == clus_type:\n",
    "        data.append(entry)\n",
    "        \n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    # Not all entries have 'answers'\n",
    "    if 'answers' in entry.keys():\n",
    "        top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                           ans_length, id_length, entry['answers'])\n",
    "        # Reconfirming no dupes\n",
    "        if len(top_ids) != len(list(set(top_ids))):\n",
    "            print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "            break\n",
    "    answers['NYU']['Uncharted'][clus_type][entry['question_id']] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncharted NYU-Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Aggregate'\n",
    "answers['NYU']['Uncharted'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] in ['AVG', 'MIN', 'MAX', 'MODE']:\n",
    "        data.append(entry)\n",
    "        \n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    # Not all entries have 'answers'\n",
    "    if 'answers' in entry.keys():\n",
    "        # Cluster Aggregate Question 94 was Removed\n",
    "        if entry['question_id'] != '94':\n",
    "            top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                               ans_length, id_length, entry['answers'])\n",
    "            # Reconfirming no dupes\n",
    "            if len(top_ids) != len(list(set(top_ids))):\n",
    "                print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "                break\n",
    "        answers['NYU']['Uncharted'][clus_type][entry['question_id']] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperion Gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncharted HG-Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers ['HG'] = {}\n",
    "answers['HG']['Uncharted'] = {}\n",
    "clus_type = 'Cluster Identification'\n",
    "answers['HG']['Uncharted'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_HG_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "data = []\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == clus_type:\n",
    "        data.append(entry)\n",
    "        \n",
    "id_pos = 0\n",
    "score_pos = 1\n",
    "ans_length = 2 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    # Not all entries have 'answers'\n",
    "    if 'answers' in entry.keys():\n",
    "        top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                           ans_length, id_length, entry['answers'])\n",
    "        # Reconfirming no dupes\n",
    "        if len(top_ids) != len(list(set(top_ids))):\n",
    "            print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "            break\n",
    "    answers['HG']['Uncharted'][clus_type][entry['question_id']] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncharted HG-Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Facet'\n",
    "answers['HG']['Uncharted'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == clus_type:\n",
    "        data.append(entry)\n",
    "        \n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    # Not all entries have 'answers'\n",
    "    if 'answers' in entry.keys():\n",
    "        top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                           ans_length, id_length, entry['answers'])\n",
    "        # Reconfirming no dupes\n",
    "        if len(top_ids) != len(list(set(top_ids))):\n",
    "            print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "            break\n",
    "    answers['HG']['Uncharted'][clus_type][entry['question_id']] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncharted HG-Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Aggregate'\n",
    "answers['HG']['Uncharted'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] in ['AVG', 'MIN', 'MAX', 'MODE']:\n",
    "        data.append(entry)\n",
    "        \n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    # Not all entries have 'answers'\n",
    "    if 'answers' in entry.keys():\n",
    "        # Cluster Aggregate Question 94 was Removed\n",
    "        if entry['question_id'] != '94':\n",
    "            top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                               ans_length, id_length, entry['answers'])\n",
    "            # Reconfirming no dupes\n",
    "            if len(top_ids) != len(list(set(top_ids))):\n",
    "                print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "                break\n",
    "        answers['HG']['Uncharted'][clus_type][entry['question_id']] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncharted JPL-Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers ['JPL'] = {}\n",
    "answers['JPL']['Uncharted'] = {}\n",
    "clus_type = 'Cluster Identification'\n",
    "answers['JPL']['Uncharted'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_JPL_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "data = []\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == clus_type:\n",
    "        data.append(entry)\n",
    "        \n",
    "id_pos = 0\n",
    "score_pos = 1\n",
    "ans_length = 2 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    # Not all entries have 'answers'\n",
    "    if 'answers' in entry.keys():\n",
    "        top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                           ans_length, id_length, entry['answers'])\n",
    "        # Reconfirming no dupes\n",
    "        if len(top_ids) != len(list(set(top_ids))):\n",
    "            print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "            break\n",
    "    answers['JPL']['Uncharted'][clus_type][entry['question_id']] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncharted JPL-Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Facet'\n",
    "answers['JPL']['Uncharted'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == clus_type:\n",
    "        data.append(entry)\n",
    "        \n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    # Not all entries have 'answers'\n",
    "    if 'answers' in entry.keys():\n",
    "        top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                           ans_length, id_length, entry['answers'])\n",
    "        # Reconfirming no dupes\n",
    "        if len(top_ids) != len(list(set(top_ids))):\n",
    "            print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "            break\n",
    "    answers['JPL']['Uncharted'][clus_type][entry['question_id']] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncharted JPL-Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Aggregate'\n",
    "answers['JPL']['Uncharted'][clus_type] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] in ['AVG', 'MIN', 'MAX', 'MODE']:\n",
    "        data.append(entry)\n",
    "        \n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    # Not all entries have 'answers'\n",
    "    if 'answers' in entry.keys():\n",
    "        # Cluster Aggregate Question 94 was Removed\n",
    "        if entry['question_id'] != '94':\n",
    "            top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                               ans_length, id_length, entry['answers'])\n",
    "            # Reconfirming no dupes\n",
    "            if len(top_ids) != len(list(set(top_ids))):\n",
    "                print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "                break\n",
    "        answers['JPL']['Uncharted'][clus_type][entry['question_id']] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Selection\n",
    "Here we look for questions that have the greatest overlap of document ids (i.e., the highest number of repeated document ids submitted by two or more teams) in the NYU index.  A greater overlap is used as an indicator that two or more teams were able to find relevant ads for the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note: the quantity reflected in this dictionary is not neccessarily \n",
    "# the \"number of duplicated ids\" since 1 document appearing 3 times would\n",
    "# count the same as 2 documents appearing 2 times each.  Perhaps a better\n",
    "# description of this quantity is something like \"the number of times\n",
    "# two different teams submitted the same document ID\".\n",
    "overlap = {}\n",
    "for qtype in seeds.keys():\n",
    "    overlap[qtype] = {}\n",
    "    for qid in seeds[qtype].keys():\n",
    "        submissions = []\n",
    "        for team in answers['NYU'].keys():\n",
    "            # Only choose questions each team answered\n",
    "            if qid not in answers['NYU'][team][qtype].keys():\n",
    "                break\n",
    "            if len(answers['NYU'][team][qtype][qid]) == 0:\n",
    "                break\n",
    "            team_sub = answers['NYU'][team][qtype][qid]\n",
    "            submissions.extend(team_sub)\n",
    "        first = len(submissions)\n",
    "        uniq_submissions = list(set(submissions))\n",
    "        second = len(uniq_submissions)\n",
    "        overlap[qtype][qid] = first - second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chosen = {}\n",
    "chosen['NYU'] = {}\n",
    "chosen['HG'] = {}\n",
    "chosen['JPL'] = {}\n",
    "for qtype in overlap.keys():\n",
    "    chosen['NYU'][qtype] = {}\n",
    "    chosen['JPL'][qtype] = {}\n",
    "    chosen['HG'][qtype] = {}\n",
    "    sorted_sub = sorted(overlap[qtype].items(), key=operator.itemgetter(1), reverse=True)\n",
    "    top_overlaps = sorted_sub[0:question_depth[qtype]]\n",
    "    for entry in top_overlaps:\n",
    "        qid = str(entry[0])\n",
    "        score = entry[1]\n",
    "        # NYU \n",
    "        chosen['NYU'][qtype][qid] = {}\n",
    "        chosen['NYU'][qtype][qid]['overlap'] = score\n",
    "        chosen['NYU'][qtype][qid]['seed'] = seeds[qtype][qid]\n",
    "        # Repeat the above uniq submissions routine for certainty\n",
    "        submissions = []\n",
    "        for team in answers['NYU'].keys():\n",
    "            chosen['NYU'][qtype][qid][team] = {}\n",
    "            chosen['NYU'][qtype][qid][team]['number submissions'] = len(answers['NYU'][team][qtype][qid])\n",
    "            submissions.extend(answers['NYU'][team][qtype][qid])\n",
    "        first = len(submissions)\n",
    "        uniq_submissions = list(set(submissions))\n",
    "        second = len(uniq_submissions)\n",
    "        if (first - second) !=  score:\n",
    "            print \"TROUBLE\"\n",
    "        chosen['NYU'][qtype][qid]['submissions'] = uniq_submissions\n",
    "        chosen['NYU'][qtype][qid]['number uniq submissions'] = len(uniq_submissions)\n",
    "        \n",
    "        # JPL\n",
    "        chosen['JPL'][qtype][qid] = {}\n",
    "        chosen['JPL'][qtype][qid]['seed'] = seeds[qtype][qid]\n",
    "        submissions = []\n",
    "        submissions.extend(answers['JPL']['Uncharted'][qtype][qid])\n",
    "        # Shouldn't have any dupes\n",
    "        first = len(submissions)\n",
    "        uniq_submissions = list(set(submissions))\n",
    "        second = len(uniq_submissions)\n",
    "        if first != second:\n",
    "            print \"DUPLICATES IN JPL SUBMISSIONS\"\n",
    "        chosen['JPL'][qtype][qid]['submissions'] = uniq_submissions\n",
    "        chosen['JPL'][qtype][qid]['number uniq submissions'] = len(uniq_submissions)\n",
    "        \n",
    "        # HG\n",
    "        chosen['HG'][qtype][qid] = {}\n",
    "        chosen['HG'][qtype][qid]['seed'] = seeds[qtype][qid]\n",
    "        submissions = []\n",
    "        submissions.extend(answers['HG']['Uncharted'][qtype][qid])\n",
    "        # Shouldn't have any dupes\n",
    "        first = len(submissions)\n",
    "        uniq_submissions = list(set(submissions))\n",
    "        second = len(uniq_submissions)\n",
    "        if first != second:\n",
    "            print \"DUPLICATES IN HG SUBMISSIONS\"\n",
    "        chosen['HG'][qtype][qid]['submissions'] = uniq_submissions\n",
    "        chosen['HG'][qtype][qid]['number uniq submissions'] = len(uniq_submissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['6126696637', '4164557000', '3345579838', '7026023157', '6477932052'])\n"
     ]
    }
   ],
   "source": [
    "# Note just as with the original set of questions, some seeds have been repeated in the chosen questions.\n",
    "# However, based on the criteria for choosing questions which are most likely to\n",
    "# produce relevant ads, it should be expected that questions with clusters represented in the DD data set\n",
    "# should repeatedly match that criteria.\n",
    "repeated_seeds = []\n",
    "for index in chosen.keys():\n",
    "    chosen_seeds = []\n",
    "    for qtype in chosen[index].keys():\n",
    "        for qid in chosen[index][qtype].keys():\n",
    "            # Check seed\n",
    "            if seeds[qtype][qid] != chosen[index][qtype][qid]['seed']:\n",
    "                print \"SEED TROUBLE\"\n",
    "            if chosen[index][qtype][qid]['seed'] in chosen_seeds:\n",
    "                repeated_seeds.append(chosen[index][qtype][qid]['seed'])\n",
    "            chosen_seeds.append(chosen[index][qtype][qid]['seed'])\n",
    "\n",
    "print set(repeated_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_file = 'chosen_questions.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(chosen, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Handoff to Uncharted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3486\n",
      "2402\n",
      " \n",
      "2534\n",
      "2337\n",
      " \n",
      "2777\n",
      "2556\n",
      " \n"
     ]
    }
   ],
   "source": [
    "temp_clusters = {}\n",
    "for index in answers.keys():\n",
    "    temp_clusters[index] = []\n",
    "    for team in answers[index].keys():\n",
    "        for qtype in answers[index][team].keys():\n",
    "            for qid in answers[index][team][qtype].keys():\n",
    "                if qid in chosen[index][qtype].keys():\n",
    "                    # Check seed\n",
    "                    seed = chosen[index][qtype][qid]['seed']\n",
    "                    seed2 = seeds[qtype][qid]\n",
    "                    if seed != seed2:\n",
    "                        print \"SEED TROUBLE\"\n",
    "                    for doc_id in answers[index][team][qtype][qid]:\n",
    "                        temp = [seed, doc_id]\n",
    "                        temp_clusters[index].append(temp)\n",
    "\n",
    "# Re-re-forming uniq IDs\n",
    "uniq_clusters = {}\n",
    "for index in temp_clusters.keys():\n",
    "    print len(temp_clusters[index])\n",
    "    uniq_clusters[index] = list(set(tuple(i) for i in temp_clusters[index]))\n",
    "    print len(uniq_clusters[index])\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_file = 'cluster_annotation_data.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(uniq_clusters, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check on Chosen Doc IDs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYU Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'NYU'\n",
    "qtype = 'Cluster Identification'\n",
    "\n",
    "# DATA\n",
    "georgetown_data = []\n",
    "file_path = submission_path + 'Georgetown/DomainDiscovery/NYU_CI.json'\n",
    "f = open(file_path, 'r')\n",
    "georgetown_data = eval(f.read())\n",
    "\n",
    "isi_data = []\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'isi-nyu-answers-dig-extractions/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_post_cluster_identification'\n",
    "                               '-parsed_fixed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "isi_data = eval(f.read())\n",
    "\n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_NYU_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == qtype:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "\n",
    "    # Georgetown\n",
    "    id_pos = 0\n",
    "    score_pos = 1\n",
    "    found = 0\n",
    "    for entry in georgetown_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if entry['id'] == qid:\n",
    "            found = 1\n",
    "            for ans in entry['answer']:\n",
    "                if ans[score_pos] > score:\n",
    "                    print \"TROUBLE!\"\n",
    "                if ans[id_pos] not in seen:\n",
    "                    count += 1\n",
    "                    if count <= 100:\n",
    "                        seen.append(ans[id_pos])\n",
    "            if len(seen) == 0:\n",
    "                print ('Georgetown', qid)\n",
    "            for doc in seen:\n",
    "                if doc not in candidates:\n",
    "                    missing.append(doc)\n",
    "                    print ('Georgetown', doc)\n",
    "    if found == 0:\n",
    "        print \"No submission Georgetown, {0}\".format(qid)\n",
    "            \n",
    "    # ISI\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in isi_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if entry['question_id'].split('-')[0] == qid:\n",
    "            found = 1\n",
    "            for ans in entry['answer']:\n",
    "                if ans[score_pos] > score:\n",
    "                    print \"TROUBLE!\"\n",
    "                if ans[id_pos] not in seen:\n",
    "                    count += 1\n",
    "                    if count <= 100:\n",
    "                        seen.append(ans[id_pos])\n",
    "            if len(seen) == 0:\n",
    "                print ('ISI', qid)\n",
    "            for doc in seen:\n",
    "                if doc not in candidates:\n",
    "                    missing.append(doc)\n",
    "                    print ('ISI', doc)\n",
    "    if found == 0:\n",
    "        print \"No submission ISI, {0}\".format(qid)\n",
    "            \n",
    "    # Uncharted\n",
    "    id_pos = 0\n",
    "    score_pos = 1\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if ans[score_pos] > score:\n",
    "                        print \"TROUBLE!\"\n",
    "                    if ans[id_pos] not in seen:\n",
    "                        count += 1\n",
    "                        if count <= 100:\n",
    "                            seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "        \n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYU Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'NYU'\n",
    "qtype = 'Cluster Facet'\n",
    "\n",
    "# DATA\n",
    "georgetown_data = []\n",
    "file_path = submission_path + 'Georgetown/DomainDiscovery/NYU_CF.json'\n",
    "f = open(file_path, 'r')\n",
    "georgetown_data = eval(f.read())\n",
    "\n",
    "isi_data = []\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'isi-nyu-answers-dig-extractions/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_post_cluster_facet'\n",
    "                               '_parsed_fixed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "isi_data = eval(f.read())\n",
    "    \n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_NYU_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == qtype:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "        \n",
    "    # Georgetown\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in georgetown_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if entry['id'] == qid:\n",
    "            found = 1\n",
    "            for ans in entry['answer']:\n",
    "                if ans[score_pos] > score:\n",
    "                    print \"TROUBLE!\"\n",
    "                if ans[id_pos] not in seen:\n",
    "                    count += 1\n",
    "                    if count <= 100:\n",
    "                        seen.append(ans[id_pos])\n",
    "            if len(seen) == 0:\n",
    "                print ('Georgetown', qid)\n",
    "            for doc in seen:\n",
    "                if doc not in candidates:\n",
    "                    missing.append(doc)\n",
    "                    print ('Georgetown', doc)\n",
    "    if found == 0:\n",
    "        print \"No submission Georgetown, {0}\".format(qid)\n",
    "            \n",
    "    # ISI\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in isi_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if entry['question_id'].split('-')[0] == qid:\n",
    "            found = 1\n",
    "            for ans in entry['answer']:\n",
    "                if ans[score_pos] > score:\n",
    "                    print \"TROUBLE!\"\n",
    "                if ans[id_pos] not in seen:\n",
    "                    count += 1\n",
    "                    if count <= 100:\n",
    "                        seen.append(ans[id_pos])\n",
    "            if len(seen) == 0:\n",
    "                print ('ISI', qid)\n",
    "            for doc in seen:\n",
    "                if doc not in candidates:\n",
    "                    missing.append(doc)\n",
    "                    print ('ISI', doc)\n",
    "    if found == 0:\n",
    "        print \"No submission ISI, {0}\".format(qid)\n",
    "            \n",
    "    # Uncharted\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if ans[score_pos] > score:\n",
    "                        print \"TROUBLE!\"\n",
    "                    if ans[id_pos] not in seen:\n",
    "                        count += 1\n",
    "                        if count <= 100:\n",
    "                            seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "                        print qid\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "\n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYU Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'NYU'\n",
    "qtype = 'Cluster Aggregate'\n",
    "\n",
    "# DATA\n",
    "georgetown_data = []\n",
    "file_path = submission_path + 'Georgetown/DomainDiscovery/NYU_aggregate.json'\n",
    "f = open(file_path, 'r')\n",
    "georgetown_data = eval(f.read())\n",
    "\n",
    "isi_data = []\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'isi-nyu-answers-dig-extractions/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_post_aggregate'\n",
    "                               '_parsed_fixed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "isi_data = eval(f.read())\n",
    "    \n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_NYU_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] in ['AVG', 'MAX', 'MIN', 'MODE']:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "        \n",
    "    # Georgetown\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in georgetown_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if entry['id'] == qid:\n",
    "            found = 1\n",
    "            for ans in entry['answer']:\n",
    "                if type(ans) == list:\n",
    "                    if len(ans) == 3:\n",
    "                        if ans[score_pos] > score:\n",
    "                            print \"TROUBLE!\"\n",
    "                        if ans[id_pos] not in seen:\n",
    "                            count += 1\n",
    "                            if count <= 100:\n",
    "                                seen.append(ans[id_pos])\n",
    "            if len(seen) == 0:\n",
    "                print ('Georgetown', qid)\n",
    "            for doc in seen:\n",
    "                if doc not in candidates:\n",
    "                    missing.append(doc)\n",
    "                    print ('Georgetown', doc)\n",
    "    if found == 0:\n",
    "        print \"No submission Georgetown, {0}\".format(qid)\n",
    "            \n",
    "    # ISI\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in isi_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if entry['question_id'].split('-')[0] == qid:\n",
    "            found = 1\n",
    "            for ans in entry['answer']:\n",
    "                if type(ans) == list:\n",
    "                    if ans[score_pos] > score:\n",
    "                        print \"TROUBLE!\"\n",
    "                    if ans[id_pos] not in seen:\n",
    "                        count += 1\n",
    "                        if count <= 100:\n",
    "                            seen.append(ans[id_pos])\n",
    "            if len(seen) == 0:\n",
    "                print ('ISI', qid)\n",
    "            for doc in seen:\n",
    "                if doc not in candidates:\n",
    "                    missing.append(doc)\n",
    "                    print ('ISI', doc)\n",
    "    if found == 0:\n",
    "        print \"No submission ISI, {0}\".format(qid)\n",
    "            \n",
    "    # Uncharted\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if type(ans) == list:\n",
    "                        if ans[score_pos] > score:\n",
    "                            print \"TROUBLE!\"\n",
    "                        if ans[id_pos] not in seen:\n",
    "                            count += 1\n",
    "                            if count <= 100:\n",
    "                                seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "                        print qid\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "\n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'JPL'\n",
    "qtype = 'Cluster Identification'\n",
    "\n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_JPL_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == qtype:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "            \n",
    "    # Uncharted\n",
    "    id_pos = 0\n",
    "    score_pos = 1\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if ans[score_pos] > score:\n",
    "                        print \"TROUBLE!\"\n",
    "                    if ans[id_pos] not in seen:\n",
    "                        count += 1\n",
    "                        if count <= 100:\n",
    "                            seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "        \n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No submission Uncharted, 26\n",
      "Confirmed, no Uncharted submission for JPL: Cluster Facet: 26\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'JPL'\n",
    "qtype = 'Cluster Facet'\n",
    "\n",
    "# DATA\n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_JPL_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == qtype:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "\n",
    "    # Uncharted\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if ans[score_pos] > score:\n",
    "                        print \"TROUBLE!\"\n",
    "                    if ans[id_pos] not in seen:\n",
    "                        count += 1\n",
    "                        if count <= 100:\n",
    "                            seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "                        print qid\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "        print \"Confirmed, no Uncharted submission for JPL: Cluster Facet: 26\"\n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'JPL'\n",
    "qtype = 'Cluster Aggregate'\n",
    "\n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_JPL_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] in ['AVG', 'MAX', 'MIN', 'MODE']:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "             \n",
    "    # Uncharted\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if type(ans) == list:\n",
    "                        if ans[score_pos] > score:\n",
    "                            print \"TROUBLE!\"\n",
    "                        if ans[id_pos] not in seen:\n",
    "                            count += 1\n",
    "                            if count <= 100:\n",
    "                                seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "                        print qid\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "\n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HG Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'HG'\n",
    "qtype = 'Cluster Identification'\n",
    "\n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_HG_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == qtype:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "            \n",
    "    # Uncharted\n",
    "    id_pos = 0\n",
    "    score_pos = 1\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if ans[score_pos] > score:\n",
    "                        print \"TROUBLE!\"\n",
    "                    if ans[id_pos] not in seen:\n",
    "                        count += 1\n",
    "                        if count <= 100:\n",
    "                            seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "        \n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HG Cluster Facect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'HG'\n",
    "qtype = 'Cluster Facet'\n",
    "\n",
    "# DATA\n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_HG_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == qtype:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "\n",
    "    # Uncharted\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if ans[score_pos] > score:\n",
    "                        print \"TROUBLE!\"\n",
    "                    if ans[id_pos] not in seen:\n",
    "                        count += 1\n",
    "                        if count <= 100:\n",
    "                            seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "                        print qid\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "        print \"Confirmed, no Uncharted submission for JPL: Cluster Facet: 26\"\n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HG Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'HG'\n",
    "qtype = 'Cluster Aggregate'\n",
    "\n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_HG_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] in ['AVG', 'MAX', 'MIN', 'MODE']:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "             \n",
    "    # Uncharted\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if type(ans) == list:\n",
    "                        if ans[score_pos] > score:\n",
    "                            print \"TROUBLE!\"\n",
    "                        if ans[id_pos] not in seen:\n",
    "                            count += 1\n",
    "                            if count <= 100:\n",
    "                                seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "                        print qid\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "\n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
