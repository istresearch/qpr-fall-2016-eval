{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pooling Depth for Each Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool_depth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_path = '../../team_submissions/'\n",
    "chosen_questions_file = 'FIRST_ROUND_chosen_questions.json'\n",
    "previous_data_file = 'FIRST_ROUND_cluster_annotation_data.json'\n",
    "\n",
    "with open(chosen_questions_file, 'r') as f:\n",
    "    chosen_questions = eval(f.read())\n",
    "    \n",
    "with open(previous_data_file, 'r') as f:\n",
    "    previous_data = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_seed(question):\n",
    "    lines = question['SPARQL'][0].split('\\n')\n",
    "    seed = str(lines[4].split(' ')[1]).strip(\"'\")\n",
    "        \n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_pool(pooling_level, id_pos, score_pos, ans_length, id_length, responses):\n",
    "    # Explicitly check for score ordering\n",
    "    score = 100\n",
    "    count = 0\n",
    "    seen = []\n",
    "    for answer in responses:\n",
    "        # Non-Array, leading elements may happen to have correct\n",
    "        # length\n",
    "        if type(answer) == list:\n",
    "            # Assume elements not matching expected length\n",
    "            # are aggregate answers\n",
    "            if len(answer) == ans_length:\n",
    "                # Confirming doc id is where we expect\n",
    "                if len(answer[id_pos]) != id_length:\n",
    "                    print \"NONSTANDARD DOC ID DETECTED\"\n",
    "                    print answer\n",
    "                    break\n",
    "                if answer[score_pos] > score:\n",
    "                    print (score, answer[score_pos])\n",
    "                    print \"RANK ORDER ISSUE\"\n",
    "                    break\n",
    "                score = answer[score_pos]\n",
    "                if answer[id_pos] not in seen:\n",
    "                    # Found another unique doc id\n",
    "                    seen.append(answer[id_pos])\n",
    "                    count += 1\n",
    "                    if count == pooling_level:\n",
    "                        # Found top N docs\n",
    "                        # Confirm no duplicates\n",
    "                        len1 = len(seen)\n",
    "                        uniq_seen = list(set(seen))\n",
    "                        if len1 != len(uniq_seen):\n",
    "                            print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "                            break\n",
    "                        return uniq_seen\n",
    "    # Or even if you don't get N uniq\n",
    "    uniq_seen = list(set(seen))\n",
    "    return uniq_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_answers = {}\n",
    "new_answers['HG'] = {}\n",
    "new_answers['JPL'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI HG-Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_answers['HG']['ISI'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Identification'\n",
    "new_answers['HG']['ISI'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'hg_all_asnwers/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_post_cluster_identification-parsed_fixed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['HG']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI HG-Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Facet'\n",
    "new_answers['HG']['ISI'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'hg_all_asnwers/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_post_cluster_facet_parsed_fixed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['HG']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI HG-Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Aggregate'\n",
    "new_answers['HG']['ISI'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'hg_all_asnwers/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_post_aggregate_parsed_fixed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['HG']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI JPL-Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_answers['JPL']['ISI'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Identification'\n",
    "new_answers['JPL']['ISI'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'jpl_answers_isi/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_cluster-identification-queries-parsed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['JPL']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI JPL-Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Facet'\n",
    "new_answers['JPL']['ISI'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'jpl_answers_isi/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_cluster-facet-queries-parsed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['JPL']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI JPL-Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Aggregate'\n",
    "new_answers['JPL']['ISI'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'jpl_answers_isi/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_aggregate-queries-parsed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['JPL']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Georgetown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_answers['HG']['Georgetown'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown HG-Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Identification'\n",
    "new_answers['HG']['Georgetown'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('Georgetown/DomainDiscovery/Georgetown_Submission'\n",
    "                               '/HG/HG_Cluster_Identification.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 0\n",
    "score_pos = 1\n",
    "ans_length = 2 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['id']\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['HG']['Georgetown'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown HG-Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Facet'\n",
    "new_answers['HG']['Georgetown'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('Georgetown/DomainDiscovery/Georgetown_Submission'\n",
    "                               '/HG/HG_Cluster_Facet.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['id']\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['HG']['Georgetown'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown HG-Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Aggregate'\n",
    "new_answers['HG']['Georgetown'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('Georgetown/DomainDiscovery/Georgetown_Submission'\n",
    "                               '/HG/HG_Aggregate.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['id']\n",
    "    if len(entry['answer']) > 0:\n",
    "        if type(entry['answer'][0]) != list:\n",
    "            agg_answ = entry['answer'][1:]\n",
    "    else:\n",
    "        agg_answ = entry['answer']\n",
    "    agg_answ.sort(key=operator.itemgetter(score_pos), reverse=True)\n",
    "    \n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, agg_answ)\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['HG']['Georgetown'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_answers['JPL']['Georgetown'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown JPL-Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Identification'\n",
    "new_answers['JPL']['Georgetown'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('Georgetown/DomainDiscovery/Georgetown_Submission'\n",
    "                               '/JPL/JPL_Cluster_Identification.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 0\n",
    "score_pos = 1\n",
    "ans_length = 2 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['id']\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['JPL']['Georgetown'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown JPL-Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Facet'\n",
    "new_answers['JPL']['Georgetown'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('Georgetown/DomainDiscovery/Georgetown_Submission'\n",
    "                               '/JPL/JPL_Cluster_Facet.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['id']\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['JPL']['Georgetown'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown JPL-Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Aggregate'\n",
    "new_answers['JPL']['Georgetown'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('Georgetown/DomainDiscovery/Georgetown_Submission'\n",
    "                               '/JPL/JPL_Aggregate.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['id']\n",
    "    if len(entry['answer']) > 0:\n",
    "        if type(entry['answer'][0]) != list:\n",
    "            agg_answ = entry['answer'][1:]\n",
    "    else:\n",
    "        agg_answ = entry['answer']\n",
    "    agg_answ.sort(key=operator.itemgetter(score_pos), reverse=True)\n",
    "    \n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, agg_answ)\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['JPL']['Georgetown'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Unique Doc IDs NOT submitted by Uncharted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_chosen_questions = {}\n",
    "\n",
    "for dataset in ['HG', 'JPL']:\n",
    "    new_chosen_questions[dataset] = {}\n",
    "    for qtype in chosen_questions[dataset].keys():\n",
    "        new_chosen_questions[dataset][qtype] = {}\n",
    "        for quest in chosen_questions[dataset][qtype].keys():\n",
    "            new_chosen_questions[dataset][qtype][quest] = {}\n",
    "            new_chosen_questions[dataset][qtype][quest]['seed'] = chosen_questions[dataset][qtype][quest]['seed']\n",
    "            \n",
    "            # PREVIOUS SUBMISSIONS\n",
    "            prev_sub = chosen_questions[dataset][qtype][quest]['submissions']\n",
    "            \n",
    "            # NEW SUBMISSIONS\n",
    "            georgetown_subs = []\n",
    "            georgetown_subs = new_answers[dataset]['Georgetown'][qtype][quest]\n",
    "            # ISI does not provide answers for all questions for all datasets\n",
    "            isi_subs = []\n",
    "            if quest in new_answers[dataset]['ISI'][qtype].keys():\n",
    "                isi_subs = new_answers[dataset]['ISI'][qtype][quest]\n",
    "            else:\n",
    "                isi_subs = []\n",
    "                #print dataset, qtype, quest\n",
    "            len_g = len(georgetown_subs)\n",
    "            len_i = len(isi_subs)\n",
    "            # List comprehension causing backwards updated\n",
    "            new_submissions = [x for x in georgetown_subs]\n",
    "            new_submissions.extend(isi_subs)\n",
    "            if len(new_submissions) != (len_g + len_i):\n",
    "                print \"TROUBLE\"\n",
    "            new_submissions = list(set(new_submissions))\n",
    "            \n",
    "            # FIND REMOVE PREVIOUSLY SUBMITTED DOCS\n",
    "            new_new_submissions = [x for x in new_submissions if x not in prev_sub]\n",
    "            \n",
    "            # POPULATE NEW CHOSEN QUESTIONS FILE\n",
    "            new_chosen_questions[dataset][qtype][quest]['Georgetown'] = {}\n",
    "            new_chosen_questions[dataset][qtype][quest]['Georgetown']['number submissions'] = len_g\n",
    "            new_chosen_questions[dataset][qtype][quest]['ISI'] = {}\n",
    "            new_chosen_questions[dataset][qtype][quest]['ISI']['number submissions'] = len_i\n",
    "            new_chosen_questions[dataset][qtype][quest]['number uniq submissions'] = len(new_new_submissions)\n",
    "            new_chosen_questions[dataset][qtype][quest]['submissions'] = new_new_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_file = 'SECOND_ROUND_chosen_questions_REWRITE.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(new_chosen_questions, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Handoff to Uncharted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3422\n",
      "2874\n",
      " \n",
      "2635\n",
      "2422\n",
      " \n"
     ]
    }
   ],
   "source": [
    "temp_clusters = {}\n",
    "for index in new_answers.keys():\n",
    "    temp_clusters[index] = []\n",
    "    for team in new_answers[index].keys():\n",
    "        for qtype in new_answers[index][team].keys():\n",
    "            for qid in new_answers[index][team][qtype].keys():\n",
    "                if qid in new_chosen_questions[index][qtype].keys():\n",
    "                    # Check seed\n",
    "                    seed = new_chosen_questions[index][qtype][qid]['seed']\n",
    "                    seed2 = chosen_questions[index][qtype][qid]['seed']\n",
    "                    if seed != seed2:\n",
    "                        print \"SEED TROUBLE\"\n",
    "                    for doc_id in new_answers[index][team][qtype][qid]:\n",
    "                        temp = [seed, doc_id]\n",
    "                        temp_clusters[index].append(temp)\n",
    "\n",
    "# Re-re-forming uniq IDs\n",
    "uniq_clusters = {}\n",
    "for index in temp_clusters.keys():\n",
    "    print len(temp_clusters[index])\n",
    "    uniq_clusters[index] = list(set(tuple(i) for i in temp_clusters[index]))\n",
    "    print len(uniq_clusters[index])\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_file = 'SECOND_ROUND_cluster_annotation_data_REWRITE.json'\n",
    "# Will be re-name manually to cluster_annotation_data.json\n",
    "# This is to avoid overwriting\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(uniq_clusters, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
