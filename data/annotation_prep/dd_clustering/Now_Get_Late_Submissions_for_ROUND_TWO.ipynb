{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pooling Depth for Each Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool_depth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_path = '../../team_submissions/'\n",
    "chosen_questions_file = 'FIRST_ROUND_chosen_questions.json'\n",
    "previous_data_file = 'FIRST_ROUND_cluster_annotation_data.json'\n",
    "\n",
    "with open(chosen_questions_file, 'r') as f:\n",
    "    chosen_questions = eval(f.read())\n",
    "    \n",
    "with open(previous_data_file, 'r') as f:\n",
    "    previous_data = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_seed(question):\n",
    "    lines = question['SPARQL'][0].split('\\n')\n",
    "    seed = str(lines[4].split(' ')[1]).strip(\"'\")\n",
    "        \n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_pool(pooling_level, id_pos, score_pos, ans_length, id_length, responses):\n",
    "    # Explicitly check for score ordering\n",
    "    score = 100\n",
    "    count = 0\n",
    "    seen = []\n",
    "    for answer in responses:\n",
    "        # Non-Array, leading elements may happen to have correct\n",
    "        # length\n",
    "        if type(answer) == list:\n",
    "            # Assume elements not matching expected length\n",
    "            # are aggregate answers\n",
    "            if len(answer) == ans_length:\n",
    "                # Confirming doc id is where we expect\n",
    "                if len(answer[id_pos]) != id_length:\n",
    "                    print \"NONSTANDARD DOC ID DETECTED\"\n",
    "                    print answer\n",
    "                    break\n",
    "                if answer[score_pos] > score:\n",
    "                    print (score, answer[score_pos])\n",
    "                    print \"RANK ORDER ISSUE\"\n",
    "                    break\n",
    "                score = answer[score_pos]\n",
    "                if answer[id_pos] not in seen:\n",
    "                    # Found another unique doc id\n",
    "                    seen.append(answer[id_pos])\n",
    "                    count += 1\n",
    "                    if count == pooling_level:\n",
    "                        # Found top N docs\n",
    "                        # Confirm no duplicates\n",
    "                        len1 = len(seen)\n",
    "                        uniq_seen = list(set(seen))\n",
    "                        if len1 != len(uniq_seen):\n",
    "                            print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "                            break\n",
    "                        return uniq_seen\n",
    "    # Or even if you don't get N uniq\n",
    "    uniq_seen = list(set(seen))\n",
    "    return uniq_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_answers = {}\n",
    "new_answers['HG'] = {}\n",
    "new_answers['JPL'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI HG-Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_answers['HG']['ISI'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Identification'\n",
    "new_answers['HG']['ISI'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'hg_all_asnwers/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_post_cluster_identification-parsed_fixed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['HG']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI HG-Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Facet'\n",
    "new_answers['HG']['ISI'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'hg_all_asnwers/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_post_cluster_facet_parsed_fixed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['HG']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI HG-Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Aggregate'\n",
    "new_answers['HG']['ISI'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'hg_all_asnwers/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_post_aggregate_parsed_fixed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['HG']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI JPL-Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_answers['JPL']['ISI'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Identification'\n",
    "new_answers['JPL']['ISI'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'jpl_answers_isi/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_cluster-identification-queries-parsed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['JPL']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI JPL-Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Facet'\n",
    "new_answers['JPL']['ISI'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'jpl_answers_isi/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_cluster-facet-queries-parsed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['JPL']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISI JPL-Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Aggregate'\n",
    "new_answers['JPL']['ISI'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('ISI/DomainDiscovery/'\n",
    "                               'jpl_answers_isi/'\n",
    "                               'properly_formatted_submissions/'\n",
    "                               'formatted_aggregate-queries-parsed_all_answers.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['question_id'].split('-')[0]\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['JPL']['ISI'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Georgetown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_answers['HG']['Georgetown'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown HG-Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Identification'\n",
    "new_answers['HG']['Georgetown'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('Georgetown/DomainDiscovery/Georgetown_Submission'\n",
    "                               '/HG/HG_Cluster_Identification.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 0\n",
    "score_pos = 1\n",
    "ans_length = 2 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['id']\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['HG']['Georgetown'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown HG-Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Facet'\n",
    "new_answers['HG']['Georgetown'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('Georgetown/DomainDiscovery/Georgetown_Submission'\n",
    "                               '/HG/HG_Cluster_Facet.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['id']\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['HG']['Georgetown'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown HG-Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Aggregate'\n",
    "new_answers['HG']['Georgetown'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('Georgetown/DomainDiscovery/Georgetown_Submission'\n",
    "                               '/HG/HG_Aggregate.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['id']\n",
    "    if len(entry['answer']) > 0:\n",
    "        if type(entry['answer'][0]) != list:\n",
    "            agg_answ = entry['answer'][1:]\n",
    "    else:\n",
    "        agg_answ = entry['answer']\n",
    "    agg_answ.sort(key=operator.itemgetter(score_pos), reverse=True)\n",
    "    \n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, agg_answ)\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['HG']['Georgetown'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_answers['JPL']['Georgetown'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown JPL-Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Identification'\n",
    "new_answers['JPL']['Georgetown'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('Georgetown/DomainDiscovery/Georgetown_Submission'\n",
    "                               '/JPL/JPL_Cluster_Identification.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 0\n",
    "score_pos = 1\n",
    "ans_length = 2 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['id']\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['JPL']['Georgetown'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown JPL-Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Facet'\n",
    "new_answers['JPL']['Georgetown'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('Georgetown/DomainDiscovery/Georgetown_Submission'\n",
    "                               '/JPL/JPL_Cluster_Facet.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['id']\n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, entry['answer'])\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['JPL']['Georgetown'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgetown JPL-Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clus_type = 'Cluster Aggregate'\n",
    "new_answers['JPL']['Georgetown'][clus_type] = {}\n",
    "\n",
    "file_path = submission_path + ('Georgetown/DomainDiscovery/Georgetown_Submission'\n",
    "                               '/JPL/JPL_Aggregate.json')\n",
    "f = open(file_path, 'r')\n",
    "data = eval(f.read())\n",
    "\n",
    "id_pos = 1\n",
    "score_pos = 2\n",
    "ans_length = 3 \n",
    "id_length = 64\n",
    "\n",
    "for entry in data:\n",
    "    qid = entry['id']\n",
    "    if len(entry['answer']) > 0:\n",
    "        if type(entry['answer'][0]) != list:\n",
    "            agg_answ = entry['answer'][1:]\n",
    "    else:\n",
    "        agg_answ = entry['answer']\n",
    "    agg_answ.sort(key=operator.itemgetter(score_pos), reverse=True)\n",
    "    \n",
    "    top_ids = top_pool(pool_depth, id_pos, score_pos,\n",
    "                       ans_length, id_length, agg_answ)\n",
    "    # Reconfirming no dupes\n",
    "    if len(top_ids) != len(list(set(top_ids))):\n",
    "        print \"PROBLEM WITH DUPLICATED DOC IDS\"\n",
    "        break\n",
    "    new_answers['JPL']['Georgetown'][clus_type][qid] = top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Unique Doc IDs NOT submitted by Uncharted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick it up here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5613F8057DE415BDE7E4B46FB5C6983BD00DBC35A0E3A3527E845CEEEDAE19E8']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_answers['HG']['Georgetown']['Cluster Aggregate']['2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Handoff to Uncharted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_clusters = {}\n",
    "for index in answers.keys():\n",
    "    temp_clusters[index] = []\n",
    "    for team in answers[index].keys():\n",
    "        for qtype in answers[index][team].keys():\n",
    "            for qid in answers[index][team][qtype].keys():\n",
    "                if qid in chosen[index][qtype].keys():\n",
    "                    # Check seed\n",
    "                    seed = chosen[index][qtype][qid]['seed']\n",
    "                    seed2 = seeds[qtype][qid]\n",
    "                    if seed != seed2:\n",
    "                        print \"SEED TROUBLE\"\n",
    "                    for doc_id in answers[index][team][qtype][qid]:\n",
    "                        temp = [seed, doc_id]\n",
    "                        temp_clusters[index].append(temp)\n",
    "\n",
    "# Re-re-forming uniq IDs\n",
    "uniq_clusters = {}\n",
    "for index in temp_clusters.keys():\n",
    "    print len(temp_clusters[index])\n",
    "    uniq_clusters[index] = list(set(tuple(i) for i in temp_clusters[index]))\n",
    "    print len(uniq_clusters[index])\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_file = 'new_cluster_annotation_data.json'\n",
    "# Will be re-name manually to cluster_annotation_data.json\n",
    "# This is to avoid overwriting\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(uniq_clusters, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check on Chosen Doc IDs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'JPL'\n",
    "qtype = 'Cluster Identification'\n",
    "\n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_JPL_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == qtype:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "            \n",
    "    # Uncharted\n",
    "    id_pos = 0\n",
    "    score_pos = 1\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if ans[score_pos] > score:\n",
    "                        print \"TROUBLE!\"\n",
    "                    if ans[id_pos] not in seen:\n",
    "                        count += 1\n",
    "                        if count <= 100:\n",
    "                            seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "        \n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Cluster Facet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'JPL'\n",
    "qtype = 'Cluster Facet'\n",
    "\n",
    "# DATA\n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_JPL_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == qtype:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "\n",
    "    # Uncharted\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if ans[score_pos] > score:\n",
    "                        print \"TROUBLE!\"\n",
    "                    if ans[id_pos] not in seen:\n",
    "                        count += 1\n",
    "                        if count <= 100:\n",
    "                            seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "                        print qid\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "        print \"Confirmed, no Uncharted submission for JPL: Cluster Facet: 26\"\n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'JPL'\n",
    "qtype = 'Cluster Aggregate'\n",
    "\n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_JPL_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] in ['AVG', 'MAX', 'MIN', 'MODE']:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "             \n",
    "    # Uncharted\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if type(ans) == list:\n",
    "                        if ans[score_pos] > score:\n",
    "                            print \"TROUBLE!\"\n",
    "                        if ans[id_pos] not in seen:\n",
    "                            count += 1\n",
    "                            if count <= 100:\n",
    "                                seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "                        print qid\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "\n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HG Cluster Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'HG'\n",
    "qtype = 'Cluster Identification'\n",
    "\n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_HG_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == qtype:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "            \n",
    "    # Uncharted\n",
    "    id_pos = 0\n",
    "    score_pos = 1\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if ans[score_pos] > score:\n",
    "                        print \"TROUBLE!\"\n",
    "                    if ans[id_pos] not in seen:\n",
    "                        count += 1\n",
    "                        if count <= 100:\n",
    "                            seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "        \n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HG Cluster Facect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'HG'\n",
    "qtype = 'Cluster Facet'\n",
    "\n",
    "# DATA\n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_HG_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] == qtype:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "\n",
    "    # Uncharted\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if ans[score_pos] > score:\n",
    "                        print \"TROUBLE!\"\n",
    "                    if ans[id_pos] not in seen:\n",
    "                        count += 1\n",
    "                        if count <= 100:\n",
    "                            seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "                        print qid\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "        print \"Confirmed, no Uncharted submission for JPL: Cluster Facet: 26\"\n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HG Cluster Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(data)\n",
    "except:\n",
    "    pass\n",
    "missing = []\n",
    "\n",
    "index = 'HG'\n",
    "qtype = 'Cluster Aggregate'\n",
    "\n",
    "uncharted_data = []\n",
    "all_data = []\n",
    "file_path = submission_path + ('Uncharted/DomainDiscovery/uncharted_HG_DD.json')\n",
    "f = open(file_path, 'r')\n",
    "all_data = eval(f.read())\n",
    "for entry in all_data:\n",
    "    if entry['questionType'] in ['AVG', 'MAX', 'MIN', 'MODE']:\n",
    "        uncharted_data.append(entry)\n",
    "\n",
    "# MAIN LOOP\n",
    "for qid in chosen[index][qtype].keys():\n",
    "    # Get list of ad candidtes\n",
    "    candidates = []\n",
    "    for clus_entry in uniq_clusters[index]:\n",
    "        if clus_entry[0] == chosen[index][qtype][qid]['seed']:\n",
    "            candidates.append(clus_entry[1])\n",
    "    if len(candidates) == 0:\n",
    "        print \"NO CANDIDATES\"\n",
    "             \n",
    "    # Uncharted\n",
    "    id_pos = 1\n",
    "    score_pos = 2\n",
    "    found = 0\n",
    "    for entry in uncharted_data:\n",
    "        seen = []\n",
    "        count = 0\n",
    "        score = 100\n",
    "        if 'answers' in entry.keys():\n",
    "            if entry['question_id'] == qid:\n",
    "                found = 1\n",
    "                for ans in entry['answers']:\n",
    "                    if type(ans) == list:\n",
    "                        if ans[score_pos] > score:\n",
    "                            print \"TROUBLE!\"\n",
    "                        if ans[id_pos] not in seen:\n",
    "                            count += 1\n",
    "                            if count <= 100:\n",
    "                                seen.append(ans[id_pos])\n",
    "                if len(seen) == 0:\n",
    "                    print ('Uncharted', qid)\n",
    "                for doc in seen:\n",
    "                    if doc not in candidates:\n",
    "                        missing.append(doc)\n",
    "                        print ('Uncharted', doc)\n",
    "                        print qid\n",
    "    if found == 0:\n",
    "        print \"No submission Uncharted, {0}\".format(qid)\n",
    "\n",
    "if len(missing) > 0:\n",
    "    print \"MISSING DOCS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chosen['NYU']['Cluster Aggregate'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
